2025-04-29 16:12:47,066 - INFO - âœ… æ—¥å¿—åˆå§‹åŒ–å®Œæˆ | è·¯å¾„: train_logs/20250429_161247.log
2025-04-29 16:12:47,090 - INFO - âœ… æ—¥å¿—åˆå§‹åŒ–å®Œæˆ | è·¯å¾„: train_logs/20250429_161247.log
2025-04-29 16:12:47,090 - INFO - ğŸ“Œ âœ¨ å½“å‰è®­ç»ƒé…ç½®
2025-04-29 16:12:47,090 - INFO -     out_dir: out
2025-04-29 16:12:47,090 - INFO -     epochs: 1
2025-04-29 16:12:47,090 - INFO -     batch_size: 32
2025-04-29 16:12:47,090 - INFO -     learning_rate: 0.0005
2025-04-29 16:12:47,090 - INFO -     device: cpu
2025-04-29 16:12:47,090 - INFO -     dtype: bfloat16
2025-04-29 16:12:47,090 - INFO -     use_wandb: False
2025-04-29 16:12:47,090 - INFO -     wandb_project: MiniQA-Pretrain
2025-04-29 16:12:47,090 - INFO -     num_workers: 1
2025-04-29 16:12:47,090 - INFO -     ddp: False
2025-04-29 16:12:47,090 - INFO -     accumulation_steps: 8
2025-04-29 16:12:47,090 - INFO -     grad_clip: 1.0
2025-04-29 16:12:47,090 - INFO -     warmup_iters: 0
2025-04-29 16:12:47,090 - INFO -     log_interval: 100
2025-04-29 16:12:47,090 - INFO -     save_interval: 100
2025-04-29 16:12:47,090 - INFO -     local_rank: -1
2025-04-29 16:12:47,090 - INFO -     hidden_size: 512
2025-04-29 16:12:47,090 - INFO -     num_hidden_layer: 8
2025-04-29 16:12:47,090 - INFO -     max_seq_len: 512
2025-04-29 16:12:47,090 - INFO -     use_moe: False
2025-04-29 16:12:47,090 - INFO -     data_path: /root/LLMDataset/pretrain_hq.jsonl
2025-04-29 16:12:47,090 - INFO -     tokenizer_path: /root/MiniQA/model/PretrainTokenizer
2025-04-29 16:12:47,135 - INFO - âš ï¸ WandB æœªå¯ç”¨æˆ–éä¸»è¿›ç¨‹
2025-04-29 16:12:47,135 - INFO - âš¡ï¸ åˆå§‹åŒ–å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ...
