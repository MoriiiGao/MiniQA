from transformers import PretrainedConfig
from typing import Optional, Union, Dict


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniQA Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

class MiniQAConfig(PretrainedConfig):
    """
    A configuration class for the littleLM language model.

    This class inherits from HuggingFace's `PretrainedConfig` and is used to store
    model architecture parameters, including transformer structure and Mixture-of-Experts (MoE)
    configurations. It supports loading from and saving to dictionaries and JSON files,
    and is compatible with the HuggingFace ecosystem.

    Attributes:
        model_type (str): Identifier for the model type. Defaults to "minimind".

    Basic Transformer Parameters:
        dim (int): Hidden size of the model. Default is 512.
        n_layers (int): Number of transformer layers. Default is 8.
        n_heads (int): Number of attention heads. Default is 8.
        n_kv_heads (int): Number of key/value heads (used for grouped attention). Default is 2.
        vocab_size (int): Vocabulary size. Default is 6400.
        hidden_dim (Optional[int]): Dimension of the feedforward layer (if None, it's computed automatically).
        multiple_of (int): Rounds hidden_dim up to a multiple of this value. Default is 64.
        norm_eps (float): Epsilon value for LayerNorm. Default is 1e-5.
        max_seq_len (int): Maximum sequence length the model supports. Default is 8192.
        rope_theta (float): Base frequency for rotary positional encoding (RoPE). Default is 1e6.
        dropout (float): Dropout probability. Default is 0.0.
        flash_attn (bool): Whether to enable Flash Attention for speed/memory efficiency. Default is True.

    MoE (Mixture-of-Experts) Parameters:
        use_moe (bool): Whether to enable MoE modules. Default is False.
        num_experts_per_tok (int): Number of experts selected per token. Default is 2.
        n_routed_experts (int): Total number of experts to route across. Default is 4.
        n_shared_experts (bool): Whether to share experts across layers. Default is True.
        scoring_func (str): Scoring function used to rank experts. Usually "softmax" or "topk". Default is "softmax".
        aux_loss_alpha (float): Weighting factor for the auxiliary loss (used in load balancing). Default is 0.1.
        seq_aux (bool): Whether to apply the auxiliary loss at sequence-level. Default is True.
        norm_topk_prob (bool): Whether to normalize the top-k probabilities during routing. Default is True.

    Methods:
        from_dict(config_dict): Class method to instantiate from a Python dict.
        to_dict(): Returns the configuration as a dict, including base class params.

    Example:
        >>> config = MiniQAConfig(dim=256, n_layers=4, use_moe=True)
        >>> print(config.to_dict())
    """
    model_type = "littleLM"

    def __init__(
        self,
        dim: int = 512,
        n_layers: int = 8,
        n_heads: int = 8,
        n_kv_heads: int = 2,
        vocab_size: int = 6400,
        hidden_dim: Optional[int] = None,
        multiple_of: int = 64,
        norm_eps: float = 1e-5,
        max_seq_len: int = 8192,
        rope_theta: float = 1e6,
        dropout: float = 0.0,
        flash_attn: bool = True,
        ##############################
        # MoE å‚æ•°
        #########################
        use_moe: bool = False,
        num_experts_per_tok: int = 2,
        n_routed_experts: int = 4,
        n_shared_experts: bool = True,
        scoring_func: str = 'softmax',
        aux_loss_alpha: float = 0.1,
        seq_aux: bool = True,
        norm_topk_prob: bool = True,
        **kwargs
    ):
        self.dim = dim                              # éšè—å±‚ç»´åº¦
        self.n_layers = n_layers                    # Transformer å±‚æ•°
        self.n_heads = n_heads                      # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
        self.n_kv_heads = n_kv_heads                # KVå¤´æ•°
        self.vocab_size = vocab_size                # è¯è¡¨å¤§å°
        self.hidden_dim = hidden_dim                # FFN éšè—å±‚ç»´åº¦
        self.multiple_of = multiple_of              # FFN éšè—å±‚å¯¹é½å•ä½
        self.norm_eps = norm_eps                    # LayerNorm çš„ eps
        self.max_seq_len = max_seq_len              # æœ€å¤§åºåˆ—é•¿åº¦
        self.rope_theta = rope_theta                # RoPE çš„é¢‘ç‡åŸºæ•°
        self.dropout = dropout                      # dropout æ¦‚ç‡
        self.flash_attn = flash_attn                # æ˜¯å¦å¯ç”¨ Flash Attention

        # MoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡

        super().__init__(**kwargs)

    @classmethod
    def from_dict(cls, config_dict: Dict) -> "MiniQAConfig":
        return cls(**config_dict)

    def to_dict(self) -> Dict:
        config = {
            "dim": self.dim,
            "n_layers": self.n_layers,
            "n_heads": self.n_heads,
            "n_kv_heads": self.n_kv_heads,
            "vocab_size": self.vocab_size,
            "hidden_dim": self.hidden_dim,
            "multiple_of": self.multiple_of,
            "norm_eps": self.norm_eps,
            "max_seq_len": self.max_seq_len,
            "rope_theta": self.rope_theta,
            "dropout": self.dropout,
            "flash_attn": self.flash_attn,
            "use_moe": self.use_moe,
            "num_experts_per_tok": self.num_experts_per_tok,
            "n_routed_experts": self.n_routed_experts,
            "n_shared_experts": self.n_shared_experts,
            "scoring_func": self.scoring_func,
            "aux_loss_alpha": self.aux_loss_alpha,
            "seq_aux": self.seq_aux,
            "norm_topk_prob": self.norm_topk_prob,
        }
        # config.update(self.to_diff_dict())  # åˆå¹¶ base class çš„å‚æ•°
        return config

# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniQA Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from typing import Optional, Tuple, List
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast

from model.model_miniqa import MiniQAConfig
from log.LoggerHelper import LoggerHelper

logger = LoggerHelper(name="Model", log_dir="train_logs")

class CausalMaskModule(nn.Module):
    def __init__(self, max_seq_len: int):
        super().__init__()

        # æ­¥éª¤1ï¼šåˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º[1, 1, max_seq_len, max_seq_len]çš„å››ç»´å¼ é‡ï¼Œå¼ é‡ä¸­å€¼å…¨éƒ¨æ˜¯-inf
        fourTensorMatrix = torch.full((1, 1, max_seq_len, max_seq_len), float('-inf'))

        # æ­¥éª¤2ï¼šå°†çŸ©é˜µçš„ä¸‹ä¸‰è§’éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ä¸»å¯¹è§’çº¿ï¼‰ç½®ä¸º0ï¼ˆä¿ç•™åŸå€¼ï¼‰ï¼Œåªä¿ç•™ä¸Šä¸‰è§’ï¼›
        causalMask = torch.triu(fourTensorMatrix)

        # åˆ›å»ºä¸€ä¸ªshapeä¸º[1, 1, max_seq_len, max_seq_len]çš„ä¸Šä¸‰è§’mask
        causal_mask = torch.triu(
            torch.full((1, 1, max_seq_len, max_seq_len), float('-inf'))
        )

        # ä½¿ç”¨register_buffer æ³¨å†Œä¸ºæ¨¡å—æŒä¹…å±æ€§ï¼ˆä¸ä¼šå‚ä¸è®­ç»ƒï¼Œä½†ä¼šä¿å­˜å’Œè½¬ç§»åˆ°GPUï¼‰
        self.register_buffer("mask", causal_mask)

    def forward(self):
        # è¿”å›maskç”¨äºattention
        # å¸¸è§ç”¨é€”ï¼šåŠ å…¥attention
        # scores = scores + causal_mask[:, :, :seq_len, :seq_len]
        return self.mask

class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization (RMSNorm)

    RMSNorm æ˜¯ä¸€ç§æ›¿ä»£ LayerNorm çš„å½’ä¸€åŒ–æ–¹å¼ï¼Œå®ƒåªåŸºäºè¾“å…¥å‘é‡çš„ L2 èŒƒæ•°ï¼ˆå‡æ–¹æ ¹ï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œ
    å»é™¤äº†å‡å€¼è®¡ç®—ï¼Œå…·æœ‰æ›´ä½çš„è®¡ç®—å¼€é”€å’Œæ›´ç¨³å®šçš„è®­ç»ƒè¡¨ç°ï¼Œå¹¿æ³›åº”ç”¨äº LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ä¸­ã€‚

    å¯¹äºè¾“å…¥å‘é‡ xï¼ŒRMSNorm è®¡ç®—ï¼š
    y = x / RMS(x) * weight
    å…¶ä¸­ RMS(x) = sqrt(mean(x^2) + eps)
    weight æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„ç¼©æ”¾å› å­

    args:
        dim(int):è¾“å…¥ç‰¹å¾ç»´åº¦
        eps(float):ä¸ºé¿å…é™¤é›¶è¯¯å·®æ·»åŠ çš„ç¨³å®šå¸¸æ•°ï¼Œé€šå¸¸ä¸º 1e-5 æˆ– 1e-6
    """
    def __init__(self, dim: int, eps: float = 1e-6):
        super(RMSNorm, self).__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å¯¹å¼ é‡æ‰§è¡ŒRMSå½’ä¸€åŒ–

        args:
            x(Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, seq_len, dim]
        returns:
            Tensor: å½’ä¸€åŒ–åçš„å¼ é‡ï¼Œå½¢çŠ¶åŒè¾“å‡º
        """

        norm = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()
        return self.weight * (x * norm)

def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)  # type: ignore
    freqs = torch.outer(t, freqs).float()  # type: ignore
    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    return pos_cis

def apply_rotary_emb(xq, xk, pos_cis):
    """
    å°†æ—‹è½¬ä½ç½®ç¼–ç åº”ç”¨äº Query å’Œ Keyã€‚

    Args:
        xq (Tensor): æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, n_heads, head_dim]
        xk (Tensor): é”®å¼ é‡ï¼Œå½¢çŠ¶åŒä¸Š
        pos_cis (Tensor): ä½ç½®æ—‹è½¬ç³»æ•°ï¼Œå½¢çŠ¶ä¸º [seq_len, head_dim]

    Returns:
        Tuple[Tensor, Tensor]: åº”ç”¨äº†æ—‹è½¬ç¼–ç çš„ xq å’Œ xk
    """
    def unite_shape(pos_cis, x):
        shape = [d if i in {1, x.ndim - 1} else 1 for i, d in enumerate(x.shape)]
        return pos_cis.view(*shape)

    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    pos_cis = unite_shape(pos_cis, xq_)
    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)

def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    å°† KV é‡å¤ä»¥åŒ¹é…å¤šå¤´ Attentionã€‚

    Args:
        x (Tensor): KV è¾“å…¥ï¼Œå½¢çŠ¶ä¸º [batch, seq_len, kv_heads, head_dim]
        n_rep (int): æ¯ä¸ª KV å¤åˆ¶çš„æ¬¡æ•°

    Returns:
        Tensor: é‡å¤åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch, seq_len, heads, head_dim]
    """
    bs, slen, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return x[:, :, :, None, :].expand(bs, slen, n_kv_heads, n_rep, head_dim).reshape(bs, slen, n_kv_heads * n_rep, head_dim)

class Attention(nn.Module):
    """
    å®ç°å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-head Attentionï¼‰æ¨¡å—ï¼Œæ”¯FlashAttentionã€KVç¼“å­˜ï¼Œå®ç°æ—‹è½¬ä½ç½®ç¼–ç 
    å¯ç”¨äºè‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸­çš„Transformer Block

    è¾“å…¥ï¼š[batch_size, seq_len, dim]
    è¾“å‡ºï¼š[batch_size, seq_len,dim]
    """
    def __init__(self, config: MiniQAConfig):
        super().__init__()

        self.n_heads = config.n_heads
        self.n_kv_heads = config.n_kv_heads or config.n_heads
        assert self.n_heads % self.n_kv_heads == 0, "headæ•°éœ€èƒ½æ•´é™¤kv_headæ•°"

        self.head_dim = config.dim // self.n_heads
        self.n_rep = self.n_heads // self.n_kv_heads

        # wq/wk/wv/wo:åˆ†åˆ«ä¸ºqueryã€keyã€valueä»¥åŠè¾“å‡ºçš„çº¿æ€§å±‚æ˜ å°„
        # head_dimï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ ç­‰äºdim // m_heads
        self.wq = nn.Linear(config.dim, self.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(config.dim, config.dim, bias=False)

        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

        self.use_flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and config.flash_attn
        if not self.use_flash:
            logger.warning("âš  Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€š Attention å®ç°ã€‚")

        # é¢„è®¡ç®—çš„ä¸Šä¸‰è§’maskï¼ˆä»…ç”¨äº causal attentionï¼‰
        # maskæ˜¯ä¸Šä¸‰è§’çŸ©é˜µï¼Œç”¨äºå®ç°å› æœMaskï¼ˆé˜²æ­¢æ¨¡å‹çœ‹æœªæ¥çš„tokenï¼‰
        self.register_buffer("mask",
                             torch.triu(torch.full((1, 1, config.max_seq_len, config.max_seq_len), float("-inf")), diagonal=1))

    def forward(
            self,
            x: torch.Tensor,
            pos_cis: torch.Tensor,
            past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
            use_cache:bool=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Attntion å‰å‘ä¼ æ’­

        args:
            x(Tensor):è¾“å…¥å¼ é‡ [batch_size, seq_lem. dim]
            pos_cis(Tensor):æ—‹è½¬ä½ç½®ç¼–ç ç³»æ•°
            past_key_value(Tuple):å†å²kvç¼“å­˜
            use_cache(bool):æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼ˆç”¨äºæ¨ç†ï¼‰

        returns:
            Tuple:
                -output:è¾“å‡ºå¼ é‡[batch_size, seq_len, dim]
                - past_kv: ç¼“å­˜çš„KVå¯¹ï¼ˆè‹¥use_cache=Trueï¼‰
        """
        bsz, seq_len, _ = x.shape

        xq = self.wq(x).view(bsz, seq_len, self.n_heads, self.head_dim)
        xk = self.wk(x).view(bsz, seq_len, self.n_kv_heads, self.head_dim)
        xv = self.wv(x).view(bsz, seq_len, self.n_kv_heads, self.head_dim)

        # å®ç°RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æœºåˆ¶ï¼Œæ¨¡å‹å¯¹é•¿è·ç¦»åºåˆ—çš„å¤„ç†èƒ½åŠ›
        xq, xk = apply_rotary_emb(xq, xk, pos_cis)

        # æ¨ç†æ—¶ä½¿ç”¨KVç¼“å­˜ åŠ é€Ÿç”Ÿæˆ
        if past_key_value is not None:
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None

        xq = xq.transpose(1, 2)  # [B, H, T, D]
        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)
        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)

        # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨pytorchçš„FlashAttention
        if self.use_flash and seq_len != 1:
            dropout_p = self.attn_dropout.p if self.training else 0.0
            output = F.scaled_dot_product_attention(
                xq, xk, xv,
                attn_mask=None,
                dropout_p=dropout_p,
                is_causal=True
            )
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores = scores + self.mask[:, :, :seq_len, :xk.shape[2]]
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv

        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        return self.resid_dropout(self.wo(output)), past_kv

class FeedForward(nn.Module):
    """
    FeedForward Module used in Transformer Blocks

    æœ¬æ¨¡å—æ˜¯ Transformer ä¸­çš„å‰é¦ˆå…¨è¿æ¥å±‚ï¼Œé‡‡ç”¨ SwiGLUï¼ˆæˆ–è¿‘ä¼¼ï¼‰ç»“æ„è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œ
    åŒ…æ‹¬ä¸€ä¸ªä¸Šå‡æŠ•å½±ï¼ˆhidden_dimï¼‰ã€éçº¿æ€§æ¿€æ´»ã€ä¸‹é™æŠ•å½±å› dim ç»´åº¦ã€‚
    å¼•å…¥ dropout æé«˜è®­ç»ƒæ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚

    è¾“å‡º y = Dropout(W2(SiLU(W1(x)) * W3(x)))

    - W1: çº¿æ€§å˜æ¢ (dim -> hidden_dim)
    - W3: Gateæ§åˆ¶ (dim -> hidden_dim)
    - W2: å›æŠ•å½± (hidden_dim -> dim)
    - SiLU: æ¿€æ´»å‡½æ•°ï¼Œç›¸æ¯” ReLU æ›´å¹³æ»‘



    """
    def __init__(self, config: MiniQAConfig):
        super().__init__()

        # è‡ªåŠ¨è®¡ç®— hidden_dimï¼ˆæ¨èç»“æ„ï¼‰å¹¶å‘ä¸Šå–æ•´åˆ° multiple_of çš„å€æ•°
        if config.hidden_dim is None:
            intermediate_dim = int((4 * config.dim * 2) / 3)
            config.hidden_dim = config.multiple_of * ((intermediate_dim + config.multiple_of - 1) // config.multiple_of)

        self.dim = config.dim
        self.hidden_dim = config.hidden_dim

        # ä¸‰ä¸ªçº¿æ€§å±‚ï¼šä¸»é€šé“ w1ï¼Œé—¨æ§é€šé“ w3ï¼Œå›æŠ•å½± w2
        self.w1 = nn.Linear(self.dim, self.hidden_dim, bias=False)
        self.w3 = nn.Linear(self.dim, self.hidden_dim, bias=False)
        self.w2 = nn.Linear(self.hidden_dim, self.dim, bias=False)

        # Dropout åœ¨è¾“å‡ºæŠ•å½±ä¹‹å
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šæ‰§è¡ŒsiLUæ¿€æ´»åçš„é—¨æ§ä¹˜æ³•ï¼Œå†é€šè¿‡è¾“å‡ºæŠ•å½±é™ç»´

        args:
            x (Tensor):è¾“å…¥å¼ é‡[batch_size, seq_len, dim]

        return:
            Tensor: è¾“å‡ºå¼ é‡ [batch_size, seq_len, dim]
        """
        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))

class MoEGate(nn.Module):
    """
    Mixture of Experts (MoE) Gate Module

    MoEé—¨æ§æ¨¡å—æ ¹æ®è¾“å…¥çš„éšè—çŠ¶æ€ `hidden_states` å¯¹ä¸“å®¶è·¯ç”±æƒé‡è¿›è¡Œè¯„åˆ†ï¼Œ
    å¹¶é€‰æ‹©å‰ top_k ä¸ªä¸“å®¶è¿›è¡Œè·¯ç”±ã€‚æ”¯æŒ softmax è¯„åˆ†æœºåˆ¶å’Œè¾…åŠ©æŸå¤±ï¼ˆaux_lossï¼‰ï¼Œ
    ä»¥æå‡è·¯ç”±çš„å‡åŒ€æ€§ã€‚



    """
    def __init__(self, config: MiniQAConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_experts = config.n_routed_experts

        self.scoring_func = config.scoring_func
        self.alpha = config.aux_loss_alpha  # auxiliary loss scale
        self.seq_aux = config.seq_aux       # æ˜¯å¦åŸºäºåºåˆ—è®¡ç®— aux loss
        self.norm_topk_prob = config.norm_topk_prob

        self.gating_dim = config.dim
        self.weight = nn.Parameter(torch.empty(self.n_experts, self.gating_dim))  # [n_experts, dim]
        self.reset_parameters()

    def reset_parameters(self):
        """
        åˆå§‹åŒ–ä¸“å®¶æƒé‡çŸ©é˜µ
        """
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­å‡½æ•°

        Args:
            hidden_states: Tensor, [batch_size, seq_len, dim]

        Returns:
            topk_idx: Tensor, [batch_size * seq_len, top_k]
            topk_weight: Tensor, [batch_size * seq_len, top_k]
            aux_loss: Tensor or float
        """
        bsz, seq_len, dim = hidden_states.shape
        hidden_states = hidden_states.view(-1, dim)  # [bsz * seq_len, dim]

        # è®¡ç®— gate scores
        logits = F.linear(hidden_states, self.weight)  # [tokens, n_experts]

        if self.scoring_func == 'softmax':
            scores = F.softmax(logits, dim=-1)
        else:
            raise NotImplementedError(f"Unsupported scoring function: {self.scoring_func}")

        # top-k é€‰æ‹©
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)  # [tokens, top_k]

        if self.top_k > 1 and self.norm_topk_prob:
            topk_weight = topk_weight / (topk_weight.sum(dim=-1, keepdim=True) + 1e-20)

        # Auxiliary Loss (è®­ç»ƒé˜¶æ®µä½¿ç”¨)
        aux_loss = self.compute_aux_loss(scores, topk_idx, bsz, seq_len) if self.training and self.alpha > 0.0 else 0.0

        return topk_idx, topk_weight, aux_loss

    def compute_aux_loss(self, scores: torch.Tensor, topk_idx: torch.Tensor, bsz: int, seq_len: int) -> torch.Tensor:
        """
        è®¡ç®—è¾…åŠ©æŸå¤± aux lossï¼Œä»¥ä¿ƒè¿› MoE è·¯ç”±çš„å‡è¡¡æ€§

        Returns:
            aux_loss: Tensor
        """
        aux_topk = self.top_k
        scores_flat = scores  # [tokens, n_experts]
        topk_idx_flat = topk_idx.view(bsz, -1)  # [bsz, seq_len * top_k]

        if self.seq_aux:
            # åŸºäºåºåˆ—è®¡ç®—aux loss
            scores_seq = scores_flat.view(bsz, seq_len, -1)  # [bsz, seq_len, n_experts]
            ce = torch.zeros(bsz, self.n_experts, device=scores.device)
            ce.scatter_add_(1, topk_idx_flat,
                            torch.ones_like(topk_idx_flat, dtype=torch.float32, device=scores.device))
            ce = ce / (seq_len * aux_topk / self.n_experts)  # normalize
            aux_loss = (ce * scores_seq.mean(dim=1)).sum(dim=1).mean() * self.alpha
        else:
            # åŸºäºå…¨å±€ token åˆ†å¸ƒ
            one_hot_mask = F.one_hot(topk_idx_flat.view(-1), num_classes=self.n_experts).float()
            ce = one_hot_mask.mean(0)  # shape: [n_experts]
            Pi = scores_flat.mean(0)
            fi = ce * self.n_experts
            aux_loss = (Pi * fi).sum() * self.alpha

        return aux_loss


class MOEFeedForward(nn.Module):
    """
    Mixture-of-Experts FeedForward æ¨¡å—

    - åŸºäº MoEGate åŠ¨æ€é€‰æ‹©å¤šä¸ªä¸“å®¶æ‰§è¡Œå‰é¦ˆæ“ä½œï¼ˆFeedForwardï¼‰
    - æ”¯æŒè®­ç»ƒé˜¶æ®µçš„ top-k è·¯ç”±ä¸è¾…åŠ©æŸå¤±è®¡ç®—
    - æ¨ç†é˜¶æ®µä»…ä½¿ç”¨ top-1 ä¸“å®¶ï¼Œæé«˜æ•ˆç‡
    - æ”¯æŒå…±äº«ä¸“å®¶è·¯å¾„ï¼ˆShared Expertsï¼‰
    """
    def __init__(self, config: MiniQAConfig):
        super().__init__()
        self.config = config
        self.experts = nn.ModuleList([
            FeedForward(config) for _ in range(config.n_routed_experts)
        ])
        self.gate = MoEGate(config)
        self.use_shared_expert = config.n_shared_experts is not None
        self.shared_expert = FeedForward(config) if self.use_shared_expert else None
        self.aux_loss = None  # type: Optional[torch.Tensor]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šæ ¹æ® gate é€‰æ‹©ä¸“å®¶å¹¶è·¯ç”±è®¡ç®—
        """
        identity = x
        bsz, seq_len, dim = x.shape
        orig_shape = x.shape

        # è·¯ç”± Gate
        topk_idx, topk_weight, aux_loss = self.gate(x)  # shape: [tokens, top_k]
        self.aux_loss = aux_loss

        x = x.view(-1, dim)  # shape: [tokens, dim]
        flat_topk_idx = topk_idx.view(-1)  # [tokens * top_k]

        if self.training:
            # è®­ç»ƒé˜¶æ®µï¼ˆå…¨è·¯å¾„ä¸“å®¶å‚ä¸ï¼‰
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)  # [tokens * top_k, dim]
            y = torch.empty_like(x, dtype=torch.float16)
            for i, expert in enumerate(self.experts):
                mask = flat_topk_idx == i
                if mask.any():
                    y[mask] = expert(x[mask]).to(y.dtype)
            y = (y.view(-1, self.config.num_experts_per_tok, dim) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.view(*orig_shape)
        else:
            # æ¨ç†é˜¶æ®µï¼ˆä»…ç”¨ top-1 è·¯ç”±ï¼‰
            y = self._moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)

        # å…±äº«ä¸“å®¶é€šè·¯ï¼ˆæ®‹å·®èåˆï¼‰
        if self.use_shared_expert:
            y = y + self.shared_expert(identity)

        return y

    @torch.no_grad()
    def _moe_infer(
        self,
        x: torch.Tensor,
        flat_expert_indices: torch.Tensor,
        flat_expert_weights: torch.Tensor
    ) -> torch.Tensor:
        """
        æ¨ç†é˜¶æ®µä½¿ç”¨ï¼šåªæ‰§è¡Œ top-1 è·¯ç”±ä¸“å®¶
        ä½¿ç”¨ scatter_add å®ç°ä¸“å®¶è·¯ç”±å’Œç»“æœåˆå¹¶

        Args:
            x: è¾“å…¥ token [tokens, dim]
            flat_expert_indices: æ¯ä¸ª token çš„ä¸“å®¶ ID [tokens]
            flat_expert_weights: æ¯ä¸ª token çš„æƒé‡ [tokens, 1]

        Returns:
            expert_cache: æ‰€æœ‰ä¸“å®¶åŠ æƒè¾“å‡ºåçš„ç»“æœ [tokens, dim]
        """
        expert_cache = torch.zeros_like(x)
        idxs = flat_expert_indices.argsort()
        sorted_indices = flat_expert_indices[idxs]
        tokens_per_expert = torch.bincount(sorted_indices).cpu().cumsum(0)

        token_idxs = idxs // self.config.num_experts_per_tok  # æ¯ä¸ªä¸“å®¶å¤„ç†å“ªäº› token

        for i, end in enumerate(tokens_per_expert):
            start = 0 if i == 0 else tokens_per_expert[i - 1]
            if start == end:
                continue
            expert = self.experts[i]
            token_ids = token_idxs[start:end]
            tokens = x[token_ids]
            outputs = expert(tokens).to(expert_cache.dtype)
            outputs.mul_(flat_expert_weights[idxs[start:end]])
            expert_cache.scatter_add_(
                0,
                token_ids.view(-1, 1).expand(-1, x.shape[1]),
                outputs
            )

        return expert_cache

class MiniQABlock(nn.Module):
    """
    MiniQABlock æ˜¯ LLM æ¨¡å‹ä¸­çš„åŸºæœ¬æ„å»ºå•å…ƒä¹‹ä¸€ï¼Œé€šå¸¸è¢«å¤šæ¬¡å †å ä»¥æ„æˆå®Œæ•´çš„ Transformer ç»“æ„ã€‚

    æ¨¡å—ç»“æ„ï¼š
    - è§„èŒƒåŒ– + å¤šå¤´æ³¨æ„åŠ›ï¼ˆAttentionï¼‰
    - æ®‹å·®è¿æ¥ï¼ˆResidualï¼‰
    - è§„èŒƒåŒ– + å‰é¦ˆç½‘ç»œï¼ˆFeedForward æˆ– MoEï¼‰
    - æ®‹å·®è¿æ¥ï¼ˆResidualï¼‰

    å‚æ•°:
        layer_id (int): å½“å‰å±‚ç¼–å·ï¼Œä»…ç”¨äºè¿½è¸ªæˆ–è°ƒè¯•ã€‚
        config (MiniQAConfig): æ¨¡å‹é…ç½®å‚æ•°ï¼ŒåŒ…å«ç»´åº¦ã€å¤´æ•°ã€æ˜¯å¦å¯ç”¨ MoE ç­‰ã€‚
    """
    def __init__(self, layer_id: int, config: MiniQAConfig):
        super().__init__()
        self.layer_id = layer_id
        self.n_heads = config.n_heads
        self.dim = config.dim
        self.head_dim = self.dim // self.n_heads

        # å¤šå¤´æ³¨æ„åŠ›æ¨¡å—
        self.attention = Attention(config)

        # RMSè§„èŒƒåŒ–ï¼ˆæ³¨æ„åŠ›å‰ï¼‰
        self.attention_norm = RMSNorm(self.dim, eps=config.norm_eps)

        # RMSè§„èŒƒåŒ–ï¼ˆFFNå‰ï¼‰
        self.ffn_norm = RMSNorm(self.dim, eps=config.norm_eps)

        # å‰é¦ˆç½‘ç»œï¼ˆæ™®é€šæˆ– MoEï¼‰
        self.feed_forward = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(
        self,
        x: torch.Tensor,
        pos_cis: torch.Tensor,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, seq_len, dim]
            pos_cis (torch.Tensor): æ—‹è½¬ä½ç½®ç¼–ç å‘é‡
            past_key_value (Optional[Tuple]): ç”¨äº KV Cache åŠ é€Ÿæ¨ç†
            use_cache (bool): æ˜¯å¦å¯ç”¨ KV Cache

        è¿”å›:
            Tuple[torch.Tensor, Optional[Tuple]]: è¾“å‡ºå¼ é‡ å’Œ å¯é€‰çš„ KV ç¼“å­˜
        """
        # æ³¨æ„åŠ›å±‚
        h_attn, past_kv = self.attention(
            self.attention_norm(x),
            pos_cis,
            past_key_value=past_key_value,
            use_cache=use_cache
        )
        h = x + h_attn  # æ®‹å·®è¿æ¥

        # å‰é¦ˆç½‘ç»œå±‚
        out = h + self.feed_forward(self.ffn_norm(h))  # æ®‹å·®è¿æ¥

        return out, past_kv

class MiniQALM(PreTrainedModel):

    config_class = MiniQAConfig

    def __init__(self, config: MiniQAConfig):
        super().__init__(config)
        self.vocab_size = config.vocab_size
        self.n_layers = config.n_layers

        # Embeddingå±‚ï¼ˆè¾“å…¥åµŒå…¥ + ä½ç½®ç¼–ç ï¼‰
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        self.dropout = nn.Dropout(config.dropout)

        # Transformer Block ç»“æ„
        self.layers = nn.ModuleList([MiniQABlock(i, config) for i in range(config.n_layers)])
        self.norm = RMSNorm(config.dim, eps=config.norm_eps)

        # è¾“å‡ºå±‚
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)
        self.output.weight = self.tok_embeddings.weight  # æƒé‡å…±äº«

        # RoPEæ—‹è½¬ä½ç½®ç¼–ç ç¼“å­˜
        self.register_buffer(
            "pos_cis",
            precompute_pos_cis(dim=config.dim // config.n_heads, theta=config.rope_theta),
            persistent=False
        )

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: bool = False,
        **kwargs
    ) -> CausalLMOutputWithPast:
        """
        æ ‡å‡†å‰å‘ä¼ æ’­ï¼Œç”¨äºè®­ç»ƒæˆ–æ¨ç†ã€‚

        Args:
            input_ids: è¾“å…¥tokenåºåˆ— (batch, seq_len)
            past_key_values: KVç¼“å­˜
            use_cache: æ˜¯å¦ç¼“å­˜KVç”¨äºæ¨ç†
            kwargs: å¯åŒ…å« start_pos

        Returns:
            CausalLMOutputWithPastï¼ŒåŒ…æ‹¬ logitsã€past_key_valuesã€aux_loss
        """
        ### 1.èµ·å§‹ä½ç½®çš„å¤„ç†
        # åœ¨ç”Ÿæˆæ—¶ï¼Œå¦‚æœæ˜¯å•tokenå¢é‡ç”Ÿæˆï¼Œéœ€è¦è®¾ç½®start_pos ç”¨æ¥å‘Šè¯‰æ¨¡å‹ ä½ç½®ç¼–ç ä»å“ªä¸ªä½ç½®å¼€å§‹ç®—
        # å¦‚æœæ˜¯è®­ç»ƒ ä¸€èˆ¬ä»0å¼€å§‹
        start_pos = kwargs.get("start_pos", 0)

        ### 2. KV ç¼“å­˜åˆå§‹åŒ–
        # å¦‚æœæ²¡æœ‰ä¼ å…¥å†å²KVç¼“å­˜ï¼Œå°±æ˜¯åˆå§‹åŒ–ä¸º[None] * å±‚æ•°
        past_key_values = past_key_values or [None] * self.n_layers

        ### 3. Token embedding + Dropout
        # input_id:[batch_size, seq_len]
        # embeddingå°†Input_idæ¯ä¸ªtokenç¼–ç æˆå‘é‡[dim]
        hidden_states = self.dropout(self.tok_embeddings(input_ids))

        ### 4. æå–ä½ç½®ç¼–ç 
        pos_cis = self.pos_cis[start_pos: start_pos + input_ids.size(1)]

        ### 5. å¤šå±‚Transformerå‰å‘ä¼ æ’­
        new_past_key_values = []
        for i, layer in enumerate(self.layers):
            hidden_states, pkv = layer(
                hidden_states,
                pos_cis,
                past_key_value=past_key_values[i],
                use_cache=use_cache
            )
            new_past_key_values.append(pkv)

        ### 6.è¾“å‡ºå±‚ï¼ˆlogitsï¼‰
        logits = self.output(self.norm(hidden_states))
        ### 7. è®¡ç®—MoEçš„è¾…åŠ©æŸå¤±
        aux_loss = sum(
            layer.feed_forward.aux_loss
            for layer in self.layers
            if isinstance(layer.feed_forward, MOEFeedForward)
        )

        return CausalLMOutputWithPast(
            logits=logits,
            past_key_values=new_past_key_values,
            loss=aux_loss,
            hidden_states=None,
            attentions=None,
            # cross_attentions=None,
            # loss=aux_loss,
        )

    @torch.inference_mode()
    def generate(
        self,
        input_ids: torch.Tensor,
        eos_token_id: int = 2,
        max_new_tokens: int = 1024,
        temperature: float = 0.75,
        top_p: float = 0.9,
        stream: bool = False,
        repetition_penalty: float = 1.0,
        use_cache: bool = True,
        pad_token_id: int = 0,
        **kwargs
    ):
        """
        æ¨ç†é˜¶æ®µç”Ÿæˆæ–‡æœ¬æ¥å£ï¼ˆæ”¯æŒæµå¼ä¸é™æ€ç”Ÿæˆï¼‰ã€‚
        """
        if stream:
            return self._stream(
                input_ids, eos_token_id, max_new_tokens,
                temperature, top_p, repetition_penalty,
                use_cache, **kwargs
            )

        generated = []
        for i in range(input_ids.size(0)):
            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)
            out = self._stream(non_pad, eos_token_id, max_new_tokens, temperature, top_p, repetition_penalty, use_cache, **kwargs)
            tokens_list = [tokens[:, -1:] for tokens in out]
            gen = torch.cat(tokens_list, dim=-1) if tokens_list else non_pad
            full_sequence = torch.cat([non_pad, gen], dim=-1)
            generated.append(full_sequence)

        max_length = max(seq.size(1) for seq in generated)
        padded = [
            torch.cat([seq, torch.full((1, max_length - seq.size(1)), pad_token_id, dtype=seq.dtype, device=seq.device)], dim=-1)
            for seq in generated
        ]
        return torch.cat(padded, dim=0)

    def _stream(
        self, input_ids, eos_token_id, max_new_tokens,
        temperature, top_p, repetition_penalty, use_cache, **kwargs
    ):
        """
        æµå¼ç”Ÿæˆå™¨ï¼ˆé€tokenè¿”å›æ–°ç”Ÿæˆçš„tokensï¼‰ã€‚
        """
        past_key_values = None
        first_token = True
        start = input_ids.shape[1]

        while input_ids.shape[1] < max_new_tokens:
            if first_token or not use_cache:
                out = self(
                    input_ids, past_key_values=past_key_values,
                    use_cache=use_cache, **kwargs
                )
                first_token = False
            else:
                out = self(
                    input_ids[:, -1:], past_key_values=past_key_values,
                    use_cache=use_cache, start_pos=input_ids.shape[1] - 1,
                    **kwargs
                )

            logits, past_key_values = out.logits[:, -1, :], out.past_key_values
            logits[:, list(set(input_ids[0].tolist()))] /= repetition_penalty
            logits = logits / (temperature + 1e-8)

            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                sorted_probs = F.softmax(sorted_logits, dim=-1)
                cumulative_probs = sorted_probs.cumsum(dim=-1)
                sorted_mask = cumulative_probs > top_p
                sorted_mask[:, 1:] = sorted_mask[:, :-1]
                sorted_mask[:, 0] = False
                mask = sorted_mask.scatter(1, sorted_indices, sorted_mask)
                logits[mask] = -float("inf")

            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            input_ids = torch.cat([input_ids, next_token], dim=1)

            yield input_ids[:, start:]

            if next_token.item() == eos_token_id:
                break

class MiniQALMLite(PreTrainedModel):
    """
    MiniQALMï¼ˆç®€åŒ–ç‰ˆï¼‰ç”¨äºè½»é‡çº§æœ¬åœ°è°ƒè¯•ï¼Œä»…ä¿ç•™ Embeddingã€Dropoutã€Linear å±‚ï¼Œ
    æ—  Transformer Blockã€æ—  RoPEã€‚
    """
    config_class = MiniQAConfig

    def __init__(self, config: MiniQAConfig):
        super().__init__(config)

        self.vocab_size = config.vocab_size
        self.n_layers = config.n_layers  # ä¿ç•™å­—æ®µï¼Œforward ä¸­ç”¨ä½œ past_kv é•¿åº¦å ä½

        # åµŒå…¥å±‚ï¼ˆtoken embeddingï¼‰
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)

        # Dropoutï¼ˆå¢å¼ºè®­ç»ƒé²æ£’æ€§ï¼‰
        self.dropout = nn.Dropout(config.dropout)

        # RMSNorm å±‚ï¼ˆä»£æ›¿ LayerNormï¼‰
        self.norm = RMSNorm(config.dim, eps=config.norm_eps)

        # è¾“å‡ºå±‚ï¼ˆæ˜ å°„åˆ° vocab logitsï¼‰
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)

        # æƒé‡å…±äº«ï¼ˆEmbedding ä¸è¾“å‡ºå±‚ï¼‰
        self.output.weight = self.tok_embeddings.weight

    def forward(
            self,
            input_ids: Optional[torch.Tensor] = None,
            past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
            use_cache: bool = False,
            **kwargs
    ) -> CausalLMOutputWithPast:
        """
        ç®€åŒ–ç‰ˆå‰å‘ä¼ æ’­ï¼šç”¨äºæœ¬åœ°è½»é‡è°ƒè¯•ï¼Œå»é™¤Transformerç»“æ„
        """
        # 1. å¤„ç†èµ·å§‹ä½ç½®
        start_pos = kwargs.get("start_pos", 0)

        # 2. æ¨¡æ‹Ÿ KV ç¼“å­˜ç»“æ„ï¼ˆåˆå§‹åŒ–ä¸ºç©ºï¼‰
        past_key_values = past_key_values or [None] * self.n_layers

        # 3. Token embedding + Dropout
        hidden_states = self.dropout(self.tok_embeddings(input_ids))

        # 4. æ¨¡æ‹Ÿä½ç½®ç¼–ç ï¼ˆä¸å†ä½¿ç”¨pos_cisï¼‰
        # ä½ å¯ä»¥å¿½ç•¥æˆ–è€…ç›´æ¥ä½¿ç”¨ embedding åçš„ hidden_states

        # 5. è·³è¿‡Transformerï¼Œåªç”¨ä¸€ä¸ªçº¿æ€§å±‚åšæ˜ å°„
        # ï¼ˆå®é™…ç­‰ä»·äºéšæœºåˆå§‹åŒ–çš„ä¸€å±‚ MLPï¼‰
        hidden_states = self.norm(hidden_states)
        logits = self.output(hidden_states)

        # 6. ä¼ªé€ MoE aux_lossï¼ˆç”¨äºä¿æŒç»“æ„ä¸€è‡´ï¼‰
        aux_loss = torch.tensor(0.0, device=input_ids.device)

        return CausalLMOutputWithPast(
            logits=logits,
            past_key_values=past_key_values,  # ä¼ªé€ ç©ºç¼“å­˜
            loss=aux_loss,
            hidden_states=None,
            attentions=None,
            # aux_loss=aux_loss,
        )

if __name__ == "__main__":
    # æµ‹è¯•mask
    mask = CausalMaskModule(max_seq_len=8)
    print(mask())


