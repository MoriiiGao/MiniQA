import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from transformers import PretrainedConfig
from typing import Optional, Union, Dict


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             DeciMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

class DeciMindConfig(PretrainedConfig):

    model_type = "littleLM"

    def __init__(
        self,
        dim: int = 512,
        n_layers: int = 8,
        n_heads: int = 8,
        n_kv_heads: int = 2,
        vocab_size: int = 6400,
        hidden_dim: Optional[int] = None,
        multiple_of: int = 64,
        norm_eps: float = 1e-5,
        max_seq_len: int = 8192,
        rope_theta: float = 1e6,
        dropout: float = 0.0,
        flash_attn: bool = True,
        ##############################
        # MoE å‚æ•°
        #########################
        use_moe: bool = False,
        num_experts_per_tok: int = 2,
        n_routed_experts: int = 4,
        n_shared_experts: bool = True,
        scoring_func: str = 'softmax',
        aux_loss_alpha: float = 0.1,
        seq_aux: bool = True,
        norm_topk_prob: bool = True,
        **kwargs
    ):
        self.dim = dim                              # éšè—å±‚ç»´åº¦
        self.n_layers = n_layers                    # Transformer å±‚æ•°
        self.n_heads = n_heads                      # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
        self.n_kv_heads = n_kv_heads                # KVå¤´æ•°
        self.vocab_size = vocab_size                # è¯è¡¨å¤§å°
        self.hidden_dim = hidden_dim                # FFN éšè—å±‚ç»´åº¦
        self.multiple_of = multiple_of              # FFN éšè—å±‚å¯¹é½å•ä½
        self.norm_eps = norm_eps                    # LayerNorm çš„ eps
        self.max_seq_len = max_seq_len              # æœ€å¤§åºåˆ—é•¿åº¦
        self.rope_theta = rope_theta                # RoPE çš„é¢‘ç‡åŸºæ•°
        self.dropout = dropout                      # dropout æ¦‚ç‡
        self.flash_attn = flash_attn                # æ˜¯å¦å¯ç”¨ Flash Attention

        # MoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡

        super().__init__(**kwargs)

    @classmethod
    def from_dict(cls, config_dict: Dict) -> "DeciMindConfig":
        return cls(**config_dict)

    def to_dict(self) -> Dict:
        config = {
            "dim": self.dim,
            "n_layers": self.n_layers,
            "n_heads": self.n_heads,
            "n_kv_heads": self.n_kv_heads,
            "vocab_size": self.vocab_size,
            "hidden_dim": self.hidden_dim,
            "multiple_of": self.multiple_of,
            "norm_eps": self.norm_eps,
            "max_seq_len": self.max_seq_len,
            "rope_theta": self.rope_theta,
            "dropout": self.dropout,
            "flash_attn": self.flash_attn,
            "use_moe": self.use_moe,
            "num_experts_per_tok": self.num_experts_per_tok,
            "n_routed_experts": self.n_routed_experts,
            "n_shared_experts": self.n_shared_experts,
            "scoring_func": self.scoring_func,
            "aux_loss_alpha": self.aux_loss_alpha,
            "seq_aux": self.seq_aux,
            "norm_topk_prob": self.norm_topk_prob,
        }
        # config.update(self.to_diff_dict())  # åˆå¹¶ base class çš„å‚æ•°
        return config

# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             DeciMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from typing import Optional, Tuple, List
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast
 
from log.LoggerHelper import LoggerHelper

logger = LoggerHelper(name="testModel", log_dir="train_logs")

def apply_rotary_emb(x_query, x_key, pos_cis):
    """
    ç”¨å¤æ•°è¿ç®—çš„æ–¹å¼å°†ä½ç½®ä¿¡æ¯ç¼–ç äºQueryå’ŒKey

    ç›®æ ‡ï¼šåœ¨ä¸å¼•å…¥é¢å¤–å‚æ•°çš„å‰æä¸‹ï¼Œå°† ç›¸å¯¹ä½ç½®å…³ç³» è‡ªç„¶çš„ä»èå…¥åˆ°Queryå’ŒKeyä¸­ï¼Œä»è€Œè®©æ³¨æ„åŠ›å…·å¤‡ä½ç½®æ„ŸçŸ¥èƒ½åŠ›

    å¤æ•°è§†è§’ä¸‹çš„ä½ç½®ç¼–ç ï¼š
        å‡è®¾head_dim = 2d,å°†æ¯ä¸¤ä¸ªç»´åº¦çœ‹æˆä¸€ä¸ªè´Ÿæ•°ï¼šZ=x_{2i} + i \dot {x_{2i+1}}
        RoPEå®šä¹‰ä¸€ä¸ªæ—‹è½¬çŸ©é˜µï¼Œå…¶æœ¬è´¨æ˜¯å°†è¿™ä¸ªå¤æ•°å‘é‡ å›ºå®šé¢‘ç‡æ—‹è½¬:RoPE(z,theta)=z\dot(e^{i theta})

    è¾“å…¥ï¼šæŸ¥è¯¢å¼ é‡(x_query) é”®å¼ é‡(key_value) æ—‹è½¬ä½ç½®ç¼–ç (pos_cis)
    è¿”å›: åº”ç”¨äº†æ—‹è½¬ä½ç½®ç¼–ç çš„x_query x_key
    """
    x_query_ = torch.view_as_complex(x_query.float().reshape(*x_query[:-1], -1, 2))
    x_key_ = torch.view_as_complex(x_key.float().reshape(*x_key[:-1], -1, -2))


class Attention(nn.Module):
    def __init__(self, config: DeciMindConfig):
        super().__init__()

        self.n_heads = config.n_heads
        self.n_kv_heads = config.n_kv_heads or config.n_heads
        # ä¿è¯Q headå¯¹KV headè¿›è¡Œç»Ÿä¸€çš„åˆ†ç»„å’Œæ˜ å°„
        # å¯èƒ½å­˜åœ¨ä¸€ä¸ªQ head å¯¹ ä¸€ä¸ª kv head
        # ä¹Ÿå¯èƒ½å¤šä¸ªQhead å¯¹ä¸€ä¸ªkv head
        assert self.n_heads % self.n_kv_heads == 0, "headæ•°éœ€è¦æ•´é™¤kv headæ•°"

        self.head_dim = config.dim // self.n_heads
        # kv headå…±äº«æ¬¡æ•° ç”¨äºMQA/GQA
        self.n_rep = self.n_head // self.n_kv_heads

        self.wq = nn.Linear(config.dim, self.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(config.dim, config.dim, bias=False)

        self.attn_dropput = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

        self.use_flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and config.flash_attn
        if not self.use_flash:
            logger.warning("âš  Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€š Attention å®ç°ã€‚")
        # æ³¨å†Œç”Ÿæˆä¸Šä¸‰è§’çŸ©é˜µ 
        # maskæ˜¯ä¸Šä¸‰è§’çŸ©é˜µ ç”¨äºå®ç°å› æœmask ï¼ˆé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        self.register_buffer("mask", 
            torch.triu(torch.full(1, 1, config.max_seq_len, max_seq_len), float("-inf"), diagonal=1))
    
    def forward(
        self,
        x: torch.Tensor,
        pos_cis = torch.Tensor,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache:bool=True
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Attentionå‰å‘ä¼ æ’­
        
        args:
            x(Tensor): input tensor [batch_size, seq_len, dim]
            pos_cis(Tensor): æ—‹è½¬ä½ç½®ç¼–ç ç³»æ•°
            past_key_value(tuple): å†å²kvç¼“å­˜
            use_cache(bool): æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼ˆç”¨äºæ¨ç†ï¼‰
        returns:
            Tuple:
             - output : output tensor of shape [batch_size, seq_len, dim]
             - past_kv: 
        """
        batch_size, seq_len, dim = x.shape
        x_query = self.wq(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
        x_key = self.wk(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        x_value = self.wv(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)


class DeciMindBlock(nn.Module):
    """
    
    """
    def __init__(self, layer_id: int, config: DeciMindConfig):
        super().__init__()

        self.layer_id = layer_id
        self.n_hedas = config.n_heads
        self.dim = config.dim
        self.head_dim = self.dim // self.n_hedas # æ³¨æ„åŠ›æ¯ä¸ªå¤´çº¬åº¦

        # å¤šå¤´æ³¨æ„åŠ›æ¨¡å—

        
class DeciMindLM(PreTrainedModel):
    def __init__(self, config: DeciMindConfig):
        super().__init__(config)
        self.vocab_size = config.vocab_size
        self.n_layers = config.n_layers

        # Embedding
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        self.dropout = nn.Dropout(config.dropout)

        # Transformer Block


if __name__ == "__main__":

    # åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º[1,1,max_seq_len,max_seq_len]çš„æ€ç»´å¼ é‡ å¼ é‡ä¸­å€¼å…¨éƒ¨æ˜¯-inf
    max_seq_len=128
    TensorMatrix = torch.full((1, 1, max_seq_len, max_seq_len), float('-inf'))
    # å°†çŸ©é˜µçš„ä¸‹ä¸‰è§’éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ä¸»å¯¹è§’çº¿ï¼‰ç½®ä¸º0ï¼ˆä¿ç•™åŸå€¼ï¼‰ï¼Œåªä¿ç•™ä¸Šä¸‰è§’
    causal_mask = torch.triu(TensorMatrix, diagonal=1)
    
    config = DeciMindConfig(
        vocab_size=1000,
        dim=256,
        n_layers=2,
        n_heads=4,
        max_seq_len=128,
        dropout=0.1,
        use_moe=True,
        n_routed_experts=4,
        num_experts_per_tok=2
    )
    # æ¨¡å‹åˆå§‹åŒ–
    # model = DeciMindLM(config)
    # model.eval()

    # æ„é€ è¾“å…¥
    batch_size = 2
    seq_len = 128
    vocab_size = config.vocab_size
    # input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    # print("input_ids.shape:", input_ids.shape)
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    print(input_ids.shape)

    # å‡åŒ€åˆ†å¸ƒè¾“å…¥
    x_query = torch.rand(batch_size, seq_len, config.n_heads, config.dim // config.n_heads) 
    x_key = torch.rand(batch_size, seq_len, config.n_heads, config.dim // config.n_heads)
    print(x_query.shape)
    pos_cis = config.rope_theta
    # [batch_size, seq_len, head_num, head_dim] -> 
    # [batch_size, seq_len, head_num, head_dim // 2, 2]
    x_query_ = x_query.float().reshape(*x_query.shape[:-1], -1, 2)
    x_key_ = x_key.float().reshape(*x_key.shape[:-1], -1, 2)
    # å°† x_query_çœ‹æˆä¸€ä¸ªå¤æ•°
    # real = x_query_[...,0] imag = x_query_[...,1]
    # x_query_complex = real + i.imag
    x_query_complex = torch.view_as_complex(x_query_)
    k_key_complex = torch.view_as_complex(x_key_)
    print(x_query_.shape)




    


    