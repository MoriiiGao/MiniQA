"""
è¯¥è„šæœ¬å®ç°äº†ä¸€ä¸ªåŸºäºGRPOç®—æ³•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¾®è°ƒä¸€ä¸ªTransformerè¯­è¨€æ¨¡å‹
ä»¥ç”Ÿæˆé«˜å¥–åŠ±æ–‡æœ¬ï¼Œä¸»è¦æµç¨‹ï¼š
[Prompt æ•°æ®] --> rollout (é‡‡æ ·ç”Ÿæˆ) --> [Episode] --> normalize_rewards --> update_policy (æ¢¯åº¦æ›´æ–°)

"""
import dataclasses
import gc
import math
from collections import defaultdict
from typing import Callable, List

import numpy as np
import torch

from dataset.data_type import Episode, MiniBatch
from model.qwen2_model import Transformer
from dataset.tokenizer import Tokenizer
import torch.nn.functional as F

@torch.no_grad()
def rollout(
    model: Transformer,
    batch: MiniBatch,
    tokenizer: Tokenizer,
    max_gen_len: int,
    num_answer_per_question: int,
    reward_function: Callable,
    device: torch.device,
    dtype: torch.dtype
) -> List[Episode]:
    """
    è½¨è¿¹é‡‡æ ·ï¼ˆé‡‡æ ·ç”Ÿæˆå›ç­”ï¼‰
    å¯¹æ¨¡å‹è¿›è¡Œé‡‡æ ·ç”Ÿæˆï¼Œå½¢æˆä¸€æ‰¹å›ç­”ï¼Œä»¥åŠè¯„ä¼°ä»–ä»¬çš„å¥–åŠ±
    Argsï¼š
        model: å½“å‰ Transformer æ¨¡å‹ã€‚
        batch: è¾“å…¥æç¤ºè¯­ï¼ˆpromptï¼‰æ‰¹æ¬¡ã€‚
        tokenizer: ç”¨äºtokenå’Œtextä¹‹é—´çš„è½¬æ¢ã€‚
        max_gen_len: æœ€é•¿ç”Ÿæˆé•¿åº¦ã€‚
        num_answer_per_question: æ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šå°‘ä¸ªå›ç­”ã€‚
        reward_function: ç”¨äºå¯¹ç”Ÿæˆå†…å®¹æ‰“åˆ†çš„å‡½æ•°ã€‚
        device, dtype: ç”¨äºæ¨¡å‹è¿è¡Œçš„è®¾å¤‡å’Œç²¾åº¦ã€‚
    å…³é”®æ­¥éª¤ï¼š
        æ„é€ å¡«å……åçš„ token å¼ é‡ã€‚
        é€æ­¥ç”Ÿæˆ tokenï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–ç”Ÿæˆç»“æŸç¬¦ã€‚
        åˆ©ç”¨ softmax + é‡‡æ ·ï¼ˆmultinomialï¼‰è·å–æ¯ä¸€æ­¥çš„ç”Ÿæˆ tokenã€‚
        åˆ¤æ–­ç”Ÿæˆæ˜¯å¦ç»“æŸï¼ˆæ˜¯å¦é‡åˆ° eos_tokenï¼‰ã€‚
        ç”Ÿæˆå®Œæˆåï¼Œå°†æ–‡æœ¬ detokenizeï¼Œè°ƒç”¨ reward å‡½æ•°è¯„ä¼°ç”Ÿæˆç»“æœã€‚
        æœ€ç»ˆè¿”å›ä¸€ä¸ªåŒ…å«æ¯æ¡è½¨è¿¹ä¿¡æ¯çš„ Episode å¯¹è±¡åˆ—è¡¨ã€‚
    """
    end_token = tokenizer.eos_token           # æ¨¡å‹è¾“å‡ºç»“æŸæ ‡è®° ç”¨äºåˆ¤æ–­è¾“å‡ºæ˜¯å¦ç»“æŸ
    end_token_id = tokenizer.eos_token_id     
    pad_token_id = tokenizer.pad_token_id
    prefix_token_ids = batch.prefix_token_ids # prompt token ids
    batch_size = len(batch.prefix) * num_answer_per_question # é—®é¢˜æ¡æ•° * æ¯ä¸ªé—®é¢˜ç”Ÿæˆç­”æ¡ˆçš„æ•°é‡
    min_prompt_len = min(len(t) for t in prefix_token_ids)
    max_prompt_len = max(len(t) for t in prefix_token_ids)
    total_len = max_gen_len + max_prompt_len # æœ€é•¿çš„prompt + æœ€å¤§çš„ç”Ÿæˆresponse
    model.init_kv_cache(
        max_batch_size=batch_size,
        max_seq_len=total_len,
        device=device,
        dtype=dtype
    )

    # åˆå§‹åŒ–ä¸€æ‰¹æ ·æœ¬æ•°æ®ï¼Œå…ˆç”¨pad_token_id pre-fill
    tokens = torch.full((batch_size, total_len), 
                        pad_token_id, dtype=torch.long, device=device)
    for idx, cur_token in enumerate(prefix_token_ids):
        offset = idx * num_answer_per_question
        for i in range(num_answer_per_question):
            # ä»å¼€å§‹ä½ç½® å°†æ¯æ¡æ•°æ®çš„promptè¿›è¡Œä¸€ä¸ªå¡«å…¥
            tokens[offset + i, :len(cur_token)] = torch.tensor(
                cur_token, dtype=torch.long, device=device
            )

    ### æ„å»ºæ¨¡å‹æ¨ç†çš„åˆå§‹çŠ¶æ€
    # å½“å‰æ¨¡å‹ç”Ÿæˆåˆ°çš„ä½ç½®ï¼ˆåˆå§‹ä¸º0ï¼‰
    prev_pos = 0 
    # æ„å»ºä¸€ä¸ªbool maskå¼ é‡ å½¢çŠ¶ä¸º[batch_size, total_len],æœ‰æ•ˆtokençš„ä½ç½®ä¸ºTrueï¼Œpaddingçš„åœ°æ–¹ä¸ºFasle
    input_text_mask = tokens != pad_token_id
    # æ£€æŸ¥æœ€å°çš„prompté•¿åº¦ä¸èƒ½è¶…è¿‡æ€»é•¿åº¦ 
    assert min_prompt_len < total_len
    # åˆå§‹åŒ–boolå‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬æ˜¯å¦å®Œæˆï¼Œå¼€å§‹æ‰€æœ‰éƒ½è®¾ä¸ºFalseï¼Œåç»­æ¯æ¬¡ç”Ÿæˆï¼Œå¦‚æœé‡åˆ°eos_tokenï¼Œåˆ™is_finishedä¸ºTrue
    is_finished = torch.zeros((batch_size, ), dtype=torch.bool, device=device)

    ### ä¸‹é¢è¦åœ¨ç»™å®šå‰ç¼€tokensçš„åŸºç¡€ä¸Šï¼Œé€æ­¥ç”Ÿæˆæ–°çš„tokenï¼ŒçŸ¥é“å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦
    for cur_pos in range(min_prompt_len, total_len):
        # ç”¨äºæ˜¾ç¤ºå½“å‰çš„è¿›åº¦çŠ¶æ€
        print(
            f"\r* Generating trajectories: {cur_pos-min_prompt_len:>4d}/{total_len-min_prompt_len:>4d}",
            flush=True,
            end="",
        )
        # æ¨¡å‹è¿›è¡Œæ··åˆæ¨ç†ï¼Œä½¿ç”¨autocastæ˜¯ä¸ºäº†æ··åˆç²¾åº¦åŠ é€Ÿ
        with torch.autocast(device_type=device.type, dtype=dtype):
            # tokens.shape:[batch_size, cur_pos - prev_pos] 
            # logits.shape:[batch_size, cur_pos - prev_pos, vocab_size]
            logits = model.inference(tokens[:, prev_pos: cur_pos])
        # ä»logitsè·å–tokenåˆ†å¸ƒ, åªå…³å¿ƒå½“å‰ç”Ÿæˆçš„æœ€åä¸€ä¸ªä½ç½®, shape:[batch_size, vocab_size]
        probs = torch.softmax(logits[:, -1], dim=-1)
        # ä»softmaxæ¦‚ç‡ä¸­ é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token = torch.multinomial(probs, num_samples=1) # [batch_size, 1]
        next_token = next_token.reshape(-1) # shape:[batch_size]
        # ä¿ç•™å·²æœ‰çš„æ–‡æœ¬ï¼Œé¿å…è¢«è¦†ç›–
        # åŸºäºinput_text_maskåœ¨cur_posçš„boolå€¼è¿›è¡Œåˆ¤æ–­ï¼Œå¦‚æœä¸ºtrueï¼Œnext_tokenå¡«å…¥tokens[:, cur_pos]
        # å¦‚æœä¸ºFalseï¼Œå¡«å…¥æ¨¡å‹é‡‡æ ·ç»“æœnext_token
        next_token = torch.where(
            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token
        )
        next_token = torch.where(is_finished, pad_token_id, next_token)
        # å†™å…¥å½“å‰tokenåˆ°tokenåºåˆ—ä¸­
        tokens[:, cur_pos] = next_token
        # åˆ¤æ–­å“ªäº›åºåˆ—å·²ç»ç”Ÿæˆç»“æŸäº†ï¼ˆå³ç”Ÿæˆäº†eos_tokenï¼‰,å¹¶æ›´æ–°is_finished çŠ¶æ€
        if end_token_id is not None:
            is_end_token = next_token == end_token_id # [batch_size] åˆ¤æ–­nextçš„token True or False
            is_generated_token = ~input_text_mask[:, cur_pos] # ~è¡¨ç¤ºç»“æœå–å shape:[batch_size]
            is_finished = is_finished | (is_end_token & is_generated_token) 
            
        # æ›´æ–°prev_pos å¹¶åˆ¤æ–­æ˜¯å¦æ‰€æœ‰sampleå·²ç»ç»“æŸ
        prev_pos = cur_pos
        if is_finished.all(): # éƒ½ä¸ºTrueï¼Œè·³å‡ºå¾ªç¯
            break

    model.del_kv_cache()
    gc.collect() # è°ƒç”¨Pythonçš„åƒåœ¾å›æ”¶æœºåˆ¶ï¼Œé‡Šæ”¾æ— ç”¨å†…å­˜å¯¹è±¡
    torch.cuda.empty_cache()
    is_finished_list = is_finished.tolist() # tensor[batch_size] -> list
    tokens_list = tokens.tolist() # [batch_size, total_len] -> list

    ### æ”¶å°¾é˜¶æ®µ
    # 1.å°†æ¨¡å‹ç”Ÿæˆçš„tokenè½¬æ¢æˆæ–‡æœ¬
    # 2.è°ƒç”¨rewardå‡½æ•°å¯¹æ–‡æœ¬è¯„åˆ†
    # 3.æ‰“åŒ…ä¸ºEpisodeå¯¹è±¡ï¼Œä¾›åç»­çš„ç­–ç•¥æ›´æ–°ä½¿ç”¨

    episodes = []
    # iè¡¨ç¤ºç¬¬å‡ ä¸ªåŸå§‹é—®é¢˜ (batch_size // num_answer_per_question)è¡¨ç¤ºpromptæ•°é‡
    for i in range(batch_size // num_answer_per_question):
        # æ¯ä¸ªpromptéƒ½æœ‰num_answer_per_questionä¸ªé—®é¢˜
        for j in range(num_answer_per_question):
            # idxè¡¨ç¤ºç¬¬iä¸ªé—®é¢˜çš„ç¬¬jä¸ªå›ç­”
            idx = i * num_answer_per_question + j
            # è·å–ç”Ÿæˆå†…å®¹çš„tokenåºåˆ— ä¸åŒ…æ‹¬prompt
            generated_token_ids = tokens_list[idx][len(batch.prefix_token_ids[i])]
            # ç§»é™¤padding token
            if pad_token_id in generated_token_ids:
                generated_token_ids = generated_token_ids[
                    : generated_token_ids.index(pad_token_id)
                ]
            # detokenizeæˆæ–‡æœ¬ å°†token is -> text str
            generated_text = tokenizer.detokenize(generated_token_ids)
            # è®¡ç®—å¥–åŠ±
            rewards = reward_function(
                response=generated_text,
                numbers=batch.numbers[i],
                target=batch.target[i],
                end_token=end_token,
            )
            # å°è£…ä¸ºä¸€æ¡å®Œæ•´çš„è½¨è¿¹
            episode = Episode(
                prefix=batch.prefix[i],    # åŸå§‹è¾“å…¥çš„prompt
                text=batch.prefix[i] + generated_text, # å®Œæ•´çš„æ–‡æœ¬ = prompt + response
                prefix_token_ids=batch.prefix_token_ids[i], # promptçš„token id
                prefix_tokens=batch.prefix_tokens[i], # promptçš„tokenå­—ç¬¦ä¸²å½¢å¼
                generated_token_ids=generated_token_ids, # responseçš„token id
                is_finished=is_finished_list[idx], # ç”Ÿæˆæ˜¯å¦ç»“æŸ
                reward=rewards["reward"], # æ‰“åˆ†ç»“æœ
                reward_info=rewards["reward_info"], # è¯¦ç»†çš„å¥–åŠ±æˆåˆ†
            )
            episodes.append(episode)
    print("\r", end=" " * 100, flush=True)
    return episodes

def normalize_rewards_per_group(episodes: List[Episode]) -> List[Episode]:
    """
    ä¸ºäº†ç¨³å®šè®­ç»ƒï¼Œå°†æ¯ç»„ç›¸åŒ(prefix)ä¸­çš„å¥–åŠ±æ ‡å‡†åŒ–
    å°†å…·æœ‰ç›¸åŒpromptçš„ä¸€ç»„å›ç­”çš„rewardåšæ ‡å‡†åŒ–ï¼Œç”Ÿæˆæ–°çš„Episodeåˆ—è¡¨
    ğŸ“Œ å®ç°ç»†èŠ‚
        å¯¹æ¯ä¸ª prefixï¼ˆé—®é¢˜ï¼‰åˆ†ç»„ã€‚
        æ¯ç»„å†…åš (reward - mean) / (std + 1e-4)ï¼Œé¿å…æ ‡å‡†å·®ä¸º0ã€‚
    ğŸ§  ä¸ºä»€ä¹ˆéœ€è¦ reward æ ‡å‡†åŒ–ï¼Ÿ
    1.å¤šä¸ªå›ç­”å¯èƒ½å…·æœ‰ä¸åŒå°ºåº¦çš„rewardï¼ˆ0.1-100ï¼‰ï¼Œä¼šå¯¼è‡´ç­–ç•¥æ›´æ–°çš„ä¸ç¨³å®š
    2.å½’ä¸€åŒ–rewardå¯ä»¥ç¡®ä¿ä¼˜åŒ–å™¨å¤„ç†çš„advantageåœ¨ä¸€ä¸ªç›¸å¯¹ä¸€è‡´çš„èŒƒå›´
    3.æŒ‰promptåˆ†ç»„æ˜¯å› ä¸ºï¼šæ¯ä¸ªpromptçš„rewardåˆ†å¸ƒå¯èƒ½ä¸åŒï¼Œå¿…é¡»å±€éƒ¨å½’ä¸€åŒ–
    """
    # åˆå§‹åŒ–æŒ‰promptåˆ†ç»„çš„å­—å…¸
    groups = defaultdict(list)
    for epsiode in episodes:
        # æŠŠæ‰€æœ‰çš„epsiodeæŒ‰prefixåˆ†ç»„
        # tuple(epsiode.prefix)æ˜¯ä¸ºäº†è®©prefixå¯ä»¥ä½œä¸ºå­—å…¸çš„key
        '''
        ç»“æœï¼š
        groups = {
            ("æ•Œäººåœ¨å“ªï¼Ÿ",): [episode1, episode2],
            ("å¤©æ°”å¦‚ä½•ï¼Ÿ",): [episode3, episode4],
        }
        '''
        groups[tuple(epsiode.prefix)].append(epsiode)

    # å¯¹æ¯ç»„groupå†…éƒ¨åšå½’ä¸€åŒ–
    output = list()
    for group in groups.values():
        group_rewards = [item.reward for item in group]
        mean_reward = np.mean(group_rewards)
        std_reward = np.std(group_rewards)
        for episode in group:
            normalized_reward = (episode.reward - mean_reward) / (std_reward + 1e-4)
            epsiode = dataclasses.replace(epsiode, reward=normalized_reward)
            output.append(epsiode)
    return output


def compute_entropy(logits: torch.Tensor) -> torch.Tensor:
    """
    è®¡ç®—æ¨¡å‹è¾“å‡ºè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒçš„ä¿¡æ¯ä¸Š
    token: [batch_size, seq_len, vocab_size]

    """
    probs = F.softmax(logits, dim=-1)
    entropy = torch.logsumexp(probs, dim=-1) - torch.sum(probs * logits, dim=-1)
    return entropy

def update_policy(model, 
                  optimizer,
                  episodes: List[Episode],
                  micro_batch_size: int,
                  pad_token_id: int,
                  max_grad_norm: float,
                  device: torch.device,
                  dtype: torch.dtype):
    """
    æ›´æ–°ç­–ç•¥ï¼šå¯¹æ¨¡å‹è¿›è¡Œä¸€æ¬¡ç­–ç•¥æ¢¯åº¦æ›´æ–° ä½¿ç”¨GRPOæ€æƒ³
    ğŸ”§ è¾“å…¥å‚æ•°
        episodes: æ‰€æœ‰ç”Ÿæˆçš„æ ·æœ¬ï¼ŒåŒ…å«æ–‡æœ¬ã€å¥–åŠ±ã€tokenã€‚
        optimizer: ä¼˜åŒ–å™¨ï¼ˆå¦‚ Adamï¼‰ã€‚
        micro_batch_size: å°æ‰¹é‡å¤„ç†å¤§å°ã€‚
        pad_token_id: ç”¨äº paddingã€‚
        max_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼ã€‚
        device, dtype: è®¾å¤‡ä¸ç²¾åº¦ã€‚
    ğŸ§  å…³é”®è®¡ç®—æ­¥éª¤
        1.è°ƒç”¨normalize_rewards_per_group()å½’ä¸€åŒ–å¥–åŠ±ã€‚
        2.æŒ‰åºåˆ—é•¿åº¦å‡åºæ’åºï¼ˆå‡å°‘ paddingï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼‰ã€‚
        3.å¯¹æ¯ä¸ª mini-batchï¼š
        4.æ„é€ è¾“å…¥ token å’Œ maskã€‚
        5.æ¨¡å‹å‰å‘ä¼ æ’­å¾—åˆ° logitsã€‚
        6.è®¡ç®— log_probsï¼šè´Ÿçš„äº¤å‰ç†µæŸå¤±ã€‚
        7.è®¡ç®— token_entropyï¼Œç”¨äºå¥–åŠ±æƒ©ç½šã€‚
        8.ç›®æ ‡å‡½æ•° = log_probs * advantageï¼ˆå³å½’ä¸€åŒ–åçš„ rewardï¼‰ã€‚
        9.åŠ æƒå¹³å‡ååå‘ä¼ æ’­ã€‚
        10.æ•´ä¸ª batch æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°ï¼ˆæ¢¯åº¦è£å‰ª+stepï¼‰ã€‚
    ğŸ“ˆ è¿”å›å€¼
        loss: å½“å‰è®­ç»ƒ lossã€‚
        grad_norm: å½“å‰æ¢¯åº¦èŒƒæ•°ã€‚
        entropy: å½“å‰ batch çš„å¹³å‡ç†µï¼ˆåæ˜ æ¨¡å‹ç¡®å®šæ€§/æ¢ç´¢æ€§ï¼‰ã€‚
    """
    episodes = normalize_rewards_per_group(episodes)
    # æŒ‰ç…§æ¯ä¸ªæ ·æœ¬çš„tokenæ€»é•¿åº¦å‡åºæ’åº
    episodes.sort(key=lambda x: len(x.prefix_token_ids) + len(x.generated_token_ids))
    # è®¡ç®—æ€»æ ·æœ¬ä¸‹éœ€è¦åˆ†å¤šå°‘micro-batchæ¥è®­ç»ƒ
    num_micro_batches = math.ceil(len(episodes) / micro_batch_size)
    # æ€»å…±è¦è®­ç»ƒçš„ç›®æ ‡tokenæ•°ï¼ˆç›®æ ‡ç”Ÿæˆéƒ¨åˆ†ï¼‰ï¼Œç”¨äºåç»­çš„å¹³å‡lossã€å¹³å‡entropy
    num_target_tokens = sum(len(episode.generated_token_ids) for episode in episodes)
    entropy = 0.0

    for i in range(0, len(episodes), micro_batch_size): # æŒ‰ç…§micro-batch-sizeåˆ’åˆ†
        print(
            f"\r* Computing policy gradient: {i:>2d}/{len(episodes):>2d}",
            flush=True,
            end="",
        )
        # è·å–ä¸€ä¸ªmicrobatchçš„episodeçš„æ•°æ®
        j = min(i + micro_batch_size, len(episodes))
        batch_episodes = episodes[i: j]
        batch_lengths = [ # è®¡ç®—æ¯ä¸ªepisodeçš„æ€»é•¿åº¦
            len(episode.prefix_token_ids) + len(episode.generated_token_ids)
            for episode in batch_episodes
        ]
        # å¯¹ä¸€ä¸ªmicro batchçš„æ•°æ®è¿›è¡Œpadding
        batch_max_length = max(batch_lengths)
        batch_token_ids = [
            episode.prefix_token_ids
            + episode.generated_token_ids 
            + [pad_token_id] * (batch_max_length - batch_lengths[i])
            for i, episode in enumerate(batch_episodes)
        ]
        # å¯¹æ¯ä¸ªmicro batchåšmaskï¼Œ åªä¿ç•™responseéƒ¨åˆ†
        batch_masks = [
            [0] * len(episode.prefix_token_ids)
            + [1] * len(episode.generated_token_ids)
            + [0] * (batch_max_length - batch_lengths[i])
            for i, episode in enumerate(episodes)
        ]
        # è®¡ç®—æ¯ä¸ªbatchçš„ä¼˜åŠ¿
        batch_advantages = [episode.reward for episode in batch_episodes]
        batch_token_ids = torch.tensor(batch_token_ids, device=device, dtype=torch.long)
        batch_masks = torch.tensor(batch_masks, device=device, dtype=torch.bool)
        batch_advantages = torch.tensor(
            batch_advantages, device=device, dtype=torch.float32
        )

        # æ··åˆç²¾åº¦æ¨ç†
        with torch.autocast(device_type=device.type, dtype=dtype):
            input_token_ids = batch_token_ids[:, :-1]
            target_token_ids = batch_token_ids[: , 1:]
            target_masks = batch_masks[:, 1:] 
            logits = model.forward(input_token_ids).float()
        
        # ç”¨æ¨¡å‹è¾“å‡ºçš„logitså’Œç›®æ ‡target_token_idsåšé€tokençš„äº¤å‰ç†µæŸå¤±ï¼Œå¹¶åŠ è´Ÿå·å¾—åˆ°log_probs
        # logits: æ¨¡å‹è¾“å‡ºçš„é¢„æµ‹å€¼ [batch_size, seq_len, vocab_size]
        # target_token_ids: ç›®æ ‡token [batch_size, seq_len]
        # log_probs: æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ [batch_size, seq_len]
        log_probs = -F.cross_entropy(
            logits.reshape(-1, logits.size(-1)), # [batch_size, seq_len, vocab_size] -> [batch_size * seq_len, vocab_size]
            target_token_ids.reshape(-1), # [batch_size ,seq_len] -> [batch_size * seq_len]
            ignore_index=pad_token_id,
            reduction="none" # ä¿ç•™æ‰€æœ‰ä½ç½®çš„loss ä¸æ±‚å’Œä¸æ±‚å¹³å‡
        ).reshape(input_token_ids.shape[0], -1)

        # è®¡ç®—å½“å‰batchçš„å¹³å‡tokençº§åˆ«ä¿¡æ¯ç†µ
        with torch.no_grad():
            # token_entropyï¼š æ¯ä¸ªtokençš„softmaxç†µå€¼ [batch_size, seq_len]
            token_entropy = compute_entropy(logits)
            # (token_entropy * target_masks).sum() / num_target_tokens ï¼š è®¡ç®—tokençš„å¹³å‡entropy
            # entropy = entropy + (...) ï¼šå¤šä¸ªmicro-batchå¯èƒ½ä¼šç´¯è®¡çš„entropy
            entropy = entropy + (token_entropy * target_masks).sum() / num_target_tokens
        
        obj = log_probs * batch_advantages[:, None]
        obj = (obj * target_masks).sum() / num_target_tokens
        loss = -obj
        loss.backward()
       
    # update the policy
    grad_norm = torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_norm=max_grad_norm
    )
    optimizer.step()
    optimizer.zero_grad(set_to_none=True)
    return {
        "loss": loss.item(),
        "grad_norm": grad_norm.item(),
        "entropy": entropy.item(),
    }