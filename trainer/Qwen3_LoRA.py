import os, sys, argparse, torch, json
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from contextlib import nullcontext
from transformers import AutoTokenizer, AutoModelForCausalLM

from model.DeciMinid_LoRA import inject_lora, LoRAConfig

def main() -> None:
    parser = argparse.ArgumentParser(description="Qwen-3 Full-Param â†’ LoRA SFT")
    # === é€šç”¨è®­ç»ƒè¶…å‚ ===
    parser.add_argument("--out_dir",          type=str, default="/root/models/Qwen3_finetune")
    parser.add_argument("--epochs",           type=int, default=2)
    parser.add_argument("--batch_size",       type=int, default=64)
    parser.add_argument("--learning_rate",    type=float, default=5e-7)
    parser.add_argument("--device",           type=str, default="cuda:0" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--dtype",            type=str, default="bfloat16")         # fp16/bf16/float32
    # === æ•°æ® & æ¨¡å‹è·¯å¾„ ===
    parser.add_argument("--model_name",       type=str, default="/root/models/Qwen_1.7",
                        help="ğŸ¤—hub repo æˆ–æœ¬åœ° ckpt ç›®å½•ï¼ˆå·²åŒ…å« config.json / pytorch_model.binï¼‰")
    parser.add_argument("--tokenizer_path",   type=str, default="/root/models/Qwen_1.7")
    parser.add_argument("--data_path",        type=str, default="/root/LLMDataset/sft_qa.jsonl")
    # === è®°å½• / åˆ†å¸ƒå¼ ===ï¼ˆå…¶ä½™å‚æ•°ä¿ç•™ï¼‰
    parser.add_argument("--log_interval",     type=int, default=100)
    # â€¦ å…¶ä½™ argparse åŒä¸Šï¼Œç•¥ â€¦

    args = parser.parse_args()
    os.makedirs(args.out_dir, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. è½½å…¥ Qwen-3 æƒé‡ & tokenizer
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"[INFO] Loading base model  â¤µ  {args.model_name}")
    torch_dtype = {"fp16": torch.float16, "bfloat16": torch.bfloat16}.get(args.dtype, torch.float32)

    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch_dtype,
        device_map="auto",             # å¤šå¡è‡ªåŠ¨åˆ†é…ï¼›å•å¡å¯ç”¨ .to(args.device)
        trust_remote_code=True
    )
    print(model)

    # è‹¥æ˜¾å¼æŒ‡å®š device_map = "auto"ï¼Œæ¨¡å‹ä¼šç›´æ¥æ”¾åˆ°å¯ç”¨ GPUï¼›
    # å¦‚æœæƒ³ä¿æŒå’ŒåŸé€»è¾‘ä¸€è‡´ï¼Œä¹Ÿå¯æ‰‹åŠ¨: model.to(args.device)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. æ³¨å…¥ LoRA
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lora_cfg = LoRAConfig(rank=4, alpha=16,
                          target_modules=("q_proj", "k_proj", "v_proj", "o_proj"))
    inject_lora(model,
                target_modules=lora_cfg.target_modules,
                rank=lora_cfg.rank,
                alpha=lora_cfg.alpha,
                dropout=lora_cfg.dropout,
                dump_txt=os.path.join(args.out_dir, "qwen3_modules.txt"))

    print("âœ… LoRA injected.  Trainable params:",
          sum(p.numel() for n, p in model.named_parameters() if p.requires_grad) / 1e6, "M")

if __name__ == "__main__":
    main()