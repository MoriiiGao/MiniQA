"""
author:
"""
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import re
import numpy as np
from pathlib import Path
import logging

import json
import random
from typing import Any, Tuple, List, Dict, Optional

import pandas as pd
from dataset.tokenizer import Tokenizer
from dataset.data_type import MiniBatch

import torch
import json
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, PreTrainedTokenizer

logger = logging.getLogger(__name__)

def merge_jsonl_files(source_path: str, target_path: str):

    with open(source_path, 'r', encoding='utf8') as src, \
        open(target_path, 'a', encoding='utf8') as tgt: 
        for line in src:
            line = line.strip()
            if not line:
                continue
            try:
                json.loads(line)
                tgt.write(line + "\n")
            except json.JSONDecodeError:
                logger.info(f"âš ï¸ è·³è¿‡éæ³• JSON è¡Œ: {line[:100]}...")

    logger.info(f"âœ… åˆå¹¶å®Œæˆï¼š{source_path} -> {target_path}")

def analyze_dataset(file_path: str, text_key: str = "text"):

    def try_open_file(path, encodings=["utf-8", "gb18030", "gbk"]):
        for enc in encodings:
            try:
                f = open(path, 'r', encoding=enc)
                # è¯•è¯»ä¸€è¡Œï¼Œç¡®ä¿ä¸ä¼šä¹±ç 
                f.readline()
                f.seek(0)
                logger.info(f"âœ… ä½¿ç”¨ç¼–ç  {enc} æˆåŠŸè¯»å–æ–‡ä»¶")
                return f
            except UnicodeDecodeError:
                continue
        raise UnicodeDecodeError("âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç æ˜¯å¦ä¸º utf-8 / gbk / gb18030")

    total_samples = 0
    total_length = 0
    max_length = 0
    min_length = float('inf')

    with try_open_file(file_path) as f:
        for line in f:
            data = json.loads(line.strip())
            text = data.get(text_key, "")
            text_length = len(text)

            total_samples += 1
            total_length += text_length
            max_length = max(max_length, text_length)
            min_length = min(min_length, text_length)

    if total_samples == 0:
        logger.info("â— æ–‡ä»¶ä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®ã€‚")
        return

    avg_length = total_length / total_samples

    logger.info(f"âœ… æ ·æœ¬æ€»æ•°: {total_samples}")
    logger.info(f"âœ… å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_length:.2f} å­—ç¬¦")
    logger.info(f"âœ… æœ€é•¿æ–‡æœ¬é•¿åº¦: {max_length} å­—ç¬¦")
    logger.info(f"âœ… æœ€çŸ­æ–‡æœ¬é•¿åº¦: {min_length} å­—ç¬¦")

class PretrainDataset(Dataset):
    """
    é¢„è®­ç»ƒæ•°æ®é›†ç±»ï¼Œç”¨äºå°†æ–‡æœ¬æ ·æœ¬åŠ è½½å¹¶ç¼–ç ä¸ºæ¨¡å‹è¾“å…¥ã€‚
    æ”¯æŒ BOS/EOS token æ·»åŠ ã€æˆªæ–­ã€å¡«å……ï¼Œä»¥åŠloss maskè®¡ç®—ã€‚
    """
    def __init__(self, data_path: str, tokenizer: PreTrainedTokenizer, max_length: int):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self._load_data(data_path)

    def _load_data(self, path: str) -> List[str]:
        samples = []
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        if 'text' in data:
                            samples.append(data['text'])
                        else:
                            logger.warning(f"Line {line_num} missing 'text' field")
                    except json.JSONDecodeError as e:
                        logger.warning(f"Line {line_num} JSON decode error: {e}")
        except FileNotFoundError:
            logger.error(f"Data file not found: {path}")
            raise
        return samples

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

        text = self.samples[index]
        #
        full_text = f"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}"

        encoding = self.tokenizer(
            full_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding.input_ids.squeeze(0)  # [seq_len]
        attention_mask = encoding.attention_mask.squeeze(0)  # [seq_len]

        X = input_ids[:-1]  # è¾“å…¥
        Y = input_ids[1:]   # æ ‡ç­¾
        loss_mask = attention_mask[1:]  # æœ‰æ•ˆéƒ¨åˆ†å‚ä¸lossè®¡ç®— (loss maskåºåˆ—ä¸­åªæœ‰1å’Œ0 1è¡¨ç¤ºå‚ä¸è®¡ç®—çš„ä½ç½®ï¼Œ0è¡¨ç¤ºä¸å‚ä¸è®¡ç®—çš„ä½ç½®)

        return X, Y, loss_mask

class SFTDataset(Dataset):
    """
    Supervised Fine-Tuning Dataset
    ç”¨äºæ„é€ DeciMindçš„ç›‘ç£å¾®è°ƒä»»åŠ¡ æ”¯æŒChatMLæ ¼å¼å¯¹è¯æ„å»º æŸå¤±æ©ç ç”Ÿæˆç­‰
    """
    def __init__(self, jsonl_path: str, tokenizer: PreTrainedTokenizer, max_length: int=1024):
        """
        Args:
            jsonl_path(str): æ•°æ®æ–‡ä»¶è·¯,è¦æ±‚æ¯ä¸€è¡Œä¸ºä¸€æ¡JSONæ ¼å¼å¯¹è¯æ•°æ®
            tokenizer: Huggingfaceæˆ–å…¼å®¹tokeinzerå®ä¾‹
            max_length(int): æœ€å¤§åºåˆ—é•¿åº¦
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self._load_data(jsonl_path)
        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids
        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids

    def __len__(self):
        return len(self.samples)
    
    def _load_data(self, jsonl_path):
        samples = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f, 1):  # æ³¨æ„è¿™é‡Œä»1å¼€å§‹ç¼–å·
                try:
                    data = json.loads(line.strip())
                    samples.append(data)
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON è§£ç é”™è¯¯ï¼å‡ºé”™æ–‡ä»¶ï¼š{jsonl_path}")
                    print(f"ğŸ§¨ å‡ºé”™è¡Œå·ï¼š{idx}")
                    print(f"ğŸ” å‡ºé”™å†…å®¹ï¼š{line.strip()}")
                    print(f"ğŸ“ é”™è¯¯ä¿¡æ¯ï¼š{e}")
                    raise e
        return samples
        
    def _create_chat_prompt(self, conversations):
        """
        æ„å»ºç¬¦åˆChatMLæ¨¡æ¿çš„æç¤ºå†…å®¹
        Args:
            conversations(List[Dict]):å¯¹è¯è½®åˆ—è¡¨ ä¾æ¬¡æ’åˆ—userå’Œassistantå‘è¨€
        Returns:
            str: æ„é€ å¥½çš„Promptæ–‡æœ¬
        """
        messages = []
        for i, turn in enumerate(conversations):
            role = "user" if i % 2 == 0 else "assistant"
            messages.append({"role": role, "content": turn["content"]})
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
    
    def _generate_loss_mask(self, input_ids):
        """
        æ ¹æ® <|im_start|>assistant å’Œ <|im_end|> æ ‡è®°ï¼Œå¯¹ assistant å›å¤å†…å®¹ç”Ÿæˆ loss maskã€‚
        """
        loss_mask = [0] * len(input_ids)
        i = 0
        max_len = min(len(input_ids), self.max_length)

        while i < max_len:
            # å°è¯•åŒ¹é… "<|im_start|>assistant"
            if input_ids[i:i + len(self.bos_id)] == self.bos_id:
                content_start = i + len(self.bos_id)
                content_end = content_start

                # åœ¨åç»­ token ä¸­å¯»æ‰¾ <|im_end|>
                while content_end < max_len:
                    if input_ids[content_end:content_end + len(self.eos_id)] == self.eos_id:
                        break
                    content_end += 1

                # åªå¯¹ assistant å›å¤å†…å®¹éƒ¨åˆ†å¯ç”¨ lossï¼ˆè·³è¿‡èµ·å§‹ tokenï¼‰
                for j in range(content_start + 1, min(content_end, max_len)):
                    loss_mask[j] = 1

                # æ›´æ–° i ä¸ºç»“å°¾ä¹‹åï¼ˆè·³è¿‡è¿™ä¸€æ®µï¼‰
                i = content_end + len(self.eos_id) if content_end < max_len else max_len
            else:
                i += 1

        return loss_mask


    def __getitem__(self, index):
        """
        è·å–å•æŒ‘è®­ç»ƒæ ·æœ¬ è¿”å›æ¨¡å‹è¾“å…¥ æ ‡ç­¾ æ©ç å·¡è§†

        Returns:
            Tuple[Tensor, Tensor, Tensor]: (input_ids, labels, loss_mask)
        """
        sample = self.samples[index] # è·å–ç¬¬iæ¡æ•°æ®
        prompt = self._create_chat_prompt(sample["conversations"]) # è·å–å¯¹è¯ æ„é€ prompt
        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]
        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))

        loss_mask = self._generate_loss_mask(input_ids)

        input_tensor = torch.tensor(input_ids[:-1], dtype=torch.long)
        label_tensor = torch.tensor(input_ids[1:], dtype=torch.long)
        mask_tensor = torch.tensor(loss_mask[1:], dtype=torch.long)

        return input_tensor, label_tensor, mask_tensor

class OLDSFTDataset(Dataset):
    def __init__(self, jsonl_path, tokenizer, max_length=1024):
        super().__init__()
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = self.load_data(jsonl_path)
        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids
        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids

    def __len__(self):
        return len(self.samples)

    def load_data(self, path):
        samples = []
        with open(path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                data = json.loads(line.strip())
                samples.append(data)
        return samples

    def _create_chat_prompt(self, conversations):
        """æ„å»ºç¬¦åˆChatMLæ ¼å¼çš„å¯¹è¯"""
        messages = []
        for i, turn in enumerate(conversations):
            role = 'user' if i % 2 == 0 else 'assistant'
            messages.append({"role": role, "content": turn['content']})
        return self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )

    def _generate_loss_mask(self, input_ids):
        loss_mask = [0] * len(input_ids)
        i = 0
        while i < len(input_ids):
            if input_ids[i:i + len(self.bos_id)] == self.bos_id:
                start = i + len(self.bos_id)
                end = start
                while end < len(input_ids):
                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:
                        break
                    end += 1
                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):
                    loss_mask[j] = 1
                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)
            else:
                i += 1
        return loss_mask

    def __getitem__(self, index):
        sample = self.samples[index]
        # æ„å»ºå¯¹è¯æç¤º
        print(sample["conversations"])
        prompt = self._create_chat_prompt(sample['conversations'])
        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]
        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))

        # ç”ŸæˆåŠ¨æ€æŸå¤±æ©ç 
        loss_mask = self._generate_loss_mask(input_ids)

        # æ„å»ºè®­ç»ƒæ•°æ®
        X = torch.tensor(input_ids[:-1], dtype=torch.long)
        Y = torch.tensor(input_ids[1:], dtype=torch.long)
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # å¯¹é½é¢„æµ‹ä½ç½®

        return X, Y, loss_mask

class MixDataset_OLD(Dataset):
    """
    LoRAå¾®è°ƒ åŒæ—¶è¯»å–é€šç”¨æ•°æ®(public)ä¸é¢†åŸŸæ•°æ®(domain)
    è®­ç»ƒæ—¶æŒ‰ `p_domain` æ¦‚ç‡ä»åŸŸå†…æ•°æ®æŠ½æ ·ï¼Œå…¶ä½™æ¦‚ç‡æŠ½é€šç”¨æ•°æ®ã€‚

    æ¯è¡Œæ•°æ®æ ¼å¼ï¼š
    {
        "conversations": [
            {"role": "user",      "content": "..."},
            {"role": "assistant", "content": "..."},
            ...
        ]
    }
    """
    def __init__(self, 
                 domain_path: str, 
                 public_path: str,
                 tokenizer: PreTrainedTokenizer,
                 max_length: int=1024,
                 p_domain: float=0.7):
        super().__init__()
        assert 0.0 <= p_domain <= 1.0, "`p_domain` must be in [0,1]"
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.p_domain = p_domain

        # è½½å…¥ä¸¤ä»½æ•°æ®
        self.domain_samples = self._load_jsonl(domain_path)
        self.public_samples = self._load_jsonl(public_path)

        # ç¼–ç special-tokens id
        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids
        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids

        # å–æœ€å¤§é•¿åº¦
        self._max_len = max(len(self.domain_samples), len(self.public_samples))
        
    @staticmethod
    def _load_jsonl(path: str) -> List[Dict]:
        samples = []
        with open(path, 'r', encoding="utf-8") as fp:
            for idx, line in enumerate(fp, 1):  # ä»ç¬¬1è¡Œå¼€å§‹è®¡æ•°
                line = line.strip()
                if not line:
                    continue
                try:
                    samples.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æé”™è¯¯ï¼šç¬¬ {idx} è¡Œï¼ŒåŸå› ï¼š{e}")
                    print(f"å‡ºé”™å†…å®¹ï¼š{line}")
                    raise e  # å¦‚æœä½ å¸Œæœ›ç¨‹åºåœæ­¢è¿è¡Œï¼Œå¦åˆ™å¯åˆ å»è¿™ä¸€è¡Œ
        return samples

    def _build_prompt(self, conversations: List[Dict]) -> str:
        msgs = [
            {"role": ("user" if i % 2 == 0 else "assistant"), "content": turn["content"]}
            for i, turn in enumerate(conversations)
        ]
        return self.tokenizer.apply_chat_template(
            msgs, tokenize=False, add_generation_prompt=False
        )

    # ç”Ÿæˆ loss-maskï¼šä»…å¯¹ <|im_start|>assistant ... <|im_end|> ä¹‹é—´ token è®¡ç®—æŸå¤±
    def _make_loss_mask(self, ids: List[int]) -> List[int]:
        mask = [0] * len(ids)
        i, n = 0, min(len(ids), self.max_length)
        while i < n:
            if ids[i : i + len(self.bos_id)] == self.bos_id:
                s = i + len(self.bos_id)
                e = s
                while e < n and ids[e : e + len(self.eos_id)] != self.eos_id:
                    e += 1
                for j in range(s + 1, min(e, n)):
                    mask[j] = 1
                i = e + len(self.eos_id)
            else:
                i += 1
        return mask
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dataset api â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def __len__(self) -> int:
        return self._max_len

    def __getitem__(self, idx: int):
        # åŠ¨æ€å†³å®šä½¿ç”¨å“ªç±»æ•°æ®
        if random.random() < self.p_domain and self.domain_samples:
            sample = self.domain_samples[idx % len(self.domain_samples)]
        else:
            sample = self.public_samples[idx % len(self.public_samples)]

        prompt      = self._build_prompt(sample["conversations"])
        input_ids   = self.tokenizer(prompt).input_ids[: self.max_length]
        pad_len     = self.max_length - len(input_ids)
        input_ids  += [self.tokenizer.pad_token_id] * pad_len

        loss_mask   = self._make_loss_mask(input_ids)

        # shift-one-token
        x = torch.tensor(input_ids[:-1],              dtype=torch.long)
        y = torch.tensor(input_ids[1:],               dtype=torch.long)
        m = torch.tensor(loss_mask[1:],               dtype=torch.long)
        return x, y, m

class MixDataset(Dataset):
    def __init__(self, domain_dataset, public_dataset, p_domain=0.7):
        self.domain_dataset = domain_dataset
        self.public_dataset = public_dataset
        self.p_domain = p_domain

    def __len__(self):
        return max(len(self.domain_dataset), len(self.public_dataset))

    def __getitem__(self, idx):
        if random.random() < self.p_domain:
            return self.domain_dataset[idx % len(self.domain_dataset)]
        else:
            return self.public_dataset[idx % len(self.public_dataset)]



def convert_json_list_to_jsonl(input_path: str, output_path: str):
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if not isinstance(data, list):
        raise ValueError("âŒ è¾“å…¥ JSON æ–‡ä»¶ç»“æ„é”™è¯¯ï¼Œæœ€å¤–å±‚åº”ä¸º listã€‚")

    with open(output_path, 'w', encoding='utf-8') as fout:
        for idx, item in enumerate(data):
            if not isinstance(item, dict) or "conversations" not in item:
                raise ValueError(f"âŒ ç¬¬ {idx + 1} é¡¹ä¸æ˜¯åˆæ³•çš„å¯¹è¯å¯¹è±¡ï¼Œç¼ºå°‘ 'conversations' é”®ã€‚")
            json_line = json.dumps(item, ensure_ascii=False)
            fout.write(json_line + '\n')

    print(f"âœ… å·²æˆåŠŸå°† {len(data)} æ¡å¯¹è¯å†™å…¥ JSONL æ–‡ä»¶: {output_path}")


SYSTEM_MESSAGE = (
    "You are a helpful assistant. You first think about the reasoning process "
    "in your mind and then provide the user with the answer."
)
USER_TEMPLATE = (
    "Using the numbers {numbers}, create an equation that equals {target}. "
    "You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. "
    "Show your work in <think> </think> tags. "
    "And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>."
)
RESPONSE_PROMPT = "Let me solve this step by step.\n<think>"

class CountdownTasksDataset(Dataset):
    """Prepare Countdown Tasks for training"""

    def __init__(
        self,
        tokenizer: Tokenizer,
        data_path: str,
        split: str = "train",
        test_size: int = 100,
    ):
        data = pd.read_parquet(Path(data_path))
        print(data.iloc[0])
        # use the last `test_size` examples for testing
        self.data = (
            data.iloc[:-test_size] if split == "train" else data.iloc[-test_size:]
        )
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data.iloc[idx].to_dict()
        item.update(self.encode_prefix(item["nums"], item["target"]))
        return item

    def encode_prefix(self, numbers: List[int], target: int):
        """Prefix is the *actual* input to the model."""
        user_message = USER_TEMPLATE.format(numbers=numbers, target=target)
        prefix = self.tokenizer.encode_chat_with_response_prompt(
            [
                {"role": "system", "content": SYSTEM_MESSAGE},
                {"role": "user", "content": user_message},
            ],
            RESPONSE_PROMPT,
        )
        tokens = self.tokenizer.tokenize(prefix)
        return {
            "prefix": prefix,
            "prefix_tokens": tokens.tokens,
            "prefix_token_ids": tokens.ids,
        }

    @staticmethod
    def collate_fn(batch: List[Dict[str, Any]]) -> MiniBatch:
        """Collate examples into a batch."""
        numbers = [item["nums"] for item in batch]
        target = [item["target"] for item in batch]
        prefix = [item["prefix"] for item in batch]
        prefix_tokens = [item["prefix_tokens"] for item in batch]
        prefix_token_ids = [item["prefix_token_ids"] for item in batch]
        return MiniBatch(
            numbers=numbers,
            target=target,
            prefix=prefix,
            prefix_tokens=prefix_tokens,
            prefix_token_ids=prefix_token_ids,
        )

class NewCountdownTasksDataset(Dataset):
    
    def __init__(self, tokenizer: Tokenizer, data_path: str, split: str = "train", test_size: int = 100):
        data = pd.read_parquet(Path(data_path))
        print("Example record:", data.iloc[0].to_dict())
        self.data = (
            data.iloc[:test_size] if split == "train" else data.iloc[-test_size:]
        ).reset_index(drop=True)
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data.iloc[idx].to_dict()

        # ä½¿ç”¨ messages æ„é€ å¯¹è¯ prompt
        messages = item["messages"]
        if isinstance(messages, str):
            import json
            messages = json.loads(messages)

        # å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œè½¬æˆ list
        if isinstance(messages, np.ndarray):
            messages = messages.tolist()

        # å¦‚æœæ˜¯å•æ¡ dictï¼ŒåŒ…è£¹æˆåˆ—è¡¨
        if isinstance(messages, dict):
            messages = [messages]

        # æœ€ç»ˆç¡®ä¿æ˜¯ listï¼Œå¦åˆ™æŠ¥é”™
        if not isinstance(messages, list):
            raise TypeError(f"`messages` should be a list, got: {type(messages)}")

        # æ·»åŠ  system message
        messages = [{"role": "system", "content": SYSTEM_MESSAGE}] + messages


        # ç¼–ç  prefix prompt
        prefix = self.tokenizer.encode_chat_with_response_prompt(
            messages,
            RESPONSE_PROMPT  # é€‚é… Qwen tokenizer é£æ ¼
        )
        tokens = self.tokenizer.tokenize(prefix)

        return {
            "prefix": prefix,
            "prefix_tokens": tokens.tokens,
            "prefix_token_ids": tokens.ids,
            "target": item.get("answer", ""),  # answer å¯ç”¨äº reward function
            "numbers": [],  # ä¿æŒ GRPO æ¥å£å…¼å®¹æ€§ï¼ˆæ— å…·ä½“ä½œç”¨ï¼‰
        }

    @staticmethod
    def collate_fn(batch: List[Dict[str, Any]]) -> MiniBatch:
        """Collate examples into a batch for GRPO."""
        return MiniBatch(
            numbers=[item["numbers"] for item in batch],
            target=[item["target"] for item in batch],
            prefix=[item["prefix"] for item in batch],
            prefix_tokens=[item["prefix_tokens"] for item in batch],
            prefix_token_ids=[item["prefix_token_ids"] for item in batch],
        )

if __name__ == "__main__":
    # merge_jsonl_files("/root/LLMDataset/MilitaryIssues.jsonl", "/root/LLMDataset/pretrain_data.jsonl")
    # dataset_path = "/root/LLMDataset/pretrain_data.jsonl"
    # analyze_dataset(dataset_path)
    # tokenizerPath = "/root/MiniQA/model/PretrainTokenizer"
    # tokenizer = AutoTokenizer.from_pretrained(tokenizerPath)
    # train_dataset = PretrainDataset(
    #     "/root/LLMDataset/MilitaryIssues.jsonl",
    #     # "/root/LLMDataset/pretrain_hq.jsonl",
    #     tokenizer,
    #     max_length=512)

    # for idx in range(len(train_dataset)):
    #     X, Y, loss_mask = train_dataset[idx]
    #     print(f"\n=== Sample {idx} ===")
    #     print("X (input):", X.tolist())
    #     print("Y (label):", Y.tolist())
    #     print("loss_mask:", loss_mask.tolist())

    #     if idx >= 4:
    #         break

    # ==============================SFTDataset===========================
    # tokenizerPath = "/root/MiniQA/model/PretrainTokenizer"
    # tokenizer = AutoTokenizer.from_pretrained(tokenizerPath)
    # sft_dataset = "/root/LLMDataset/decimin_dataset/sft_512.jsonl"
    # sftdataset = OLDSFTDataset(sft_dataset, tokenizer)

    # for i in range(100):
    #     X, Y, mask = sftdataset[i]
        # print(f"Sample {i}")
        # print("Input IDs:", X.shape)
        # print("Labels:", Y.shape)
        # print(mask)
        # print("Loss Mask:", mask.sum().item(), "positions used for loss")

    # ==============================Mix SFTDataset===========================
    tokenizerPath = "/root/MiniQA/model/PretrainTokenizer"
    tokenizer = AutoTokenizer.from_pretrained(tokenizerPath)
    Domain_path = "/root/LLMDataset/decimin_dataset/scenario.jsonl"
    Public_path = "/root/LLMDataset/decimin_dataset/sft_mini_512.jsonl"
    
    # mixdata = MixDataset(
    #     domain_path = Domain_path,
    #     public_path = Public_path,
    #     tokenizer   = tokenizer,
    #     max_length  = 1024,
    #     p_domain    = 0.7,           # 70% é¢†åŸŸ / 30% é€šç”¨
    # )

    # for i in range(100):
    #     X, Y, mask = mixdata[i]

    # convert_json_list_to_jsonl(Domain_path, "/root/LLMDataset/decimin_dataset/scenario.jsonl")